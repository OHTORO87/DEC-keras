# 기록 1
* GPU 할당 번호를 바꿈
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
* mnist 테스트
========================================================================================================================
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 784)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               392500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 10)                20010
_________________________________________________________________
clustering (ClusteringLayer) (None, 10)                100
=================================================================
Total params: 1,665,110
Trainable params: 1,665,110
Non-trainable params: 0
_________________________________________________________________
Update interval 140
Save interval 1365
Initializing cluster centers with k-means.


Iter 5880: acc = 0.96059, nmi = 0.90227, ari = 0.91529  ; loss= 0.11461
delta_label  0.0009 < tol  0.001
Reached tolerance threshold. Stopping training.
saving model to: results/DEC_model_final.h5
/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/sklearn/utils/linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.
  warnings.warn(
acc: 0.9605857142857143
clustering time:  180.99016213417053
========================================================================================================================



# 기록 2 ( score 편집 )
========================================================================================================================
* 네이버 score 편집
    2로 나누고 > round > 0점인 경우 1로 바꿈 >int
* 사용한 코드

    # 데이터 부르기
    df_naver = csv_reader("naver_reiview_preprocess_latest_merge")

    # 모든값을 반으로 나눈후 반올림
    df_naver['score'] = round(df_naver['score'] / 2)

    # 0값이 있는 경우 1로 변경
    df_naver.loc[df_naver['score'] == 0, 'score'] = 1

    # score float > int 변경
    df_dtype_int = df_naver.astype({'score': int})

    # 저장
    csv_save(df_dtype_int, 'naver_reiview_preprocess_latest_merge')
========================================================================================================================
* 1점을 2로 나눈후 round(0.5) == 0 현상 발생 > 0점을 1점으로 변경




# 기록 3
=======================================================================================================================
* 사용한 코드
    df_all = csv_reader(file_name) # csv 파일 load
    df_preprocess = df_all.loc[:, ['score','preprocessed_review']] # 점수와 전처리된 리뷰만 가져옴
    df_clean = df_preprocess.dropna(axis=0) # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True) # 인덱스 재정렬

* 결과
    ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /tmp/pycharm_project_367/DEC-keras/datasets.py
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 3001 entries, 0 to 3000
    Data columns (total 2 columns):
     #   Column               Non-Null Count  Dtype
    ---  ------               --------------  -----
     0   score                3001 non-null   int64
     1   preprocessed_review  3001 non-null   object
    dtypes: int64(1), object(1)
    memory usage: 47.0+ KB
          score                                preprocessed_review
    0         4                                                 좋음
    1         4                                                 좋음
    2         5                                       전체적으로 만족도 높음
    3         2                                               그럭저럭
    4         4                                                 최고
    ...     ...                                                ...
    2996      2  하루 묵기는 무난합니다 중앙난방인 지방에 냉방을 요청했는데 난방 시즌이어서 냉방은 ...
    2997      2  박일 숙박했는데 너무 깔끔하고 전망도 아주 좋았습니다 층 디럭스룸 배정받았는데 웬만...
    2998      2  편히 쉬다 가기 좋은 곳 위치도 좋고 무엇보다 침구류가 좋아서 편안하게 휴식 취할 ...
    2999      2  멤버십 가입 후 처음 이용했는데 정말 퇴실하기 싫을 정도로 좋았습니다 분이 친절하게...
    3000      2  한국 갈 때마다 롯데시티나 신라스테이를 이용했는데 왜 글래드를 몰랐는지 아쉽다 앞으...

    [3001 rows x 2 columns]
    None
=======================================================================================================================
* 5000개의 리뷰 데이터가 있는 csv 파일에서 nan 값을 제외한 3000개의 데이터를 남겼다.



# 기록 4 (okt 까지 한번 돌려봄)
=======================================================================================================================
* 사용한 코드
    '''
    데이터 로딩
    '''
    print('Loading data...') # 데이터 로딩
    df_all = csv_reader(file_name) # csv 파일 load
    df_preprocess = df_all.loc[:, ['score','preprocessed_review']] # 점수와 전처리된 리뷰만 가져옴
    df_clean = df_preprocess.dropna(axis=0) # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True) # 인덱스 재정렬
    x1, y1 = df_reindex['preprocessed_review'], df_reindex['score'] # x 리뷰, y 점수

    '''
    okt
    '''
    print('Vectorizing sequence data...') # 데이터 벡터화
    tokenizer = okt_morph(x1) # 전처리된 리뷰 데이터 토크나이징
    return tokenizer


* 결과
    Loading data...
    Vectorizing sequence data...
    100%|██████████████████████████████████████| 3001/3001 [00:14<00:00, 211.03it/s]
    Traceback (most recent call last):
      File "/tmp/pycharm_project_367/DEC-keras/datasets.py", line 389, in <module>
        x, y = load_crawleing_reviews("naver_review_test_data")
    ValueError: too many values to unpack (expected 2)
=======================================================================================================================
* okt_morph() 함수에 시리즈 말고 데이터프레임으로 넣어야 될듯???


# 기록 5
=======================================================================================================================
* 사용한 코드
    # 형태소 분석 함수
    def okt_morph(dataframe):
        # 범위 지정 가능
        df_pre = dataframe['preprocessed_review']
        df_corpus = df_pre[:]

        clean_words = []
        for i in tqdm(df_corpus):
            ok = okt.pos(i)
            words = []
            for word in ok:
                if word[1] not in ['Josa', 'Eomi', 'Punctuation', 'Suffix']:  # 조사, 어미, 구두점이 있는 것은 포함 시키지 않음
                    words.append(word[0])
            clean_words.append(words)

        return clean_words

* 결과
    100%|██████████████████████████████████████| 3001/3001 [00:15<00:00, 196.22it/s]
    리뷰 데이터 : [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '사',
    '통', '팔', '달', '교통', '편리'], ['강력', '추천'], ['매우', '나쁨'], ['구', '성비', '갑', '최고', '호텔'], .. 생략
=======================================================================================================================
* okt 형태소 분류 까지 완료


# 기록 6 (word2vec 예제파일을 따라하며 데이터가 출력되는지 확인해보기)
           * "좋음" 과 비슷한 단어 추출하기
========================================================================================================================
* 사용한 코드
    '''
    word2vec
    '''
    print('Vectorizing sequence data...')  # 데이터 벡터화
    model = Word2Vec(sentences=tokenizer, size=100, window=5, min_count=5, workers=4, sg=0)
    model_result = model.wv.most_similar("좋음")
    print(model_result)


* 결과
    Vectorizing sequence data...
    [('내부', 0.9974992275238037), ('좋습니다', 0.9973698854446411), ('깔끔했고', 0.9973666667938232), ('또한', 0.997318685054779),
    ('친절합니다', 0.9972794651985168), ('넓은', 0.9972636699676514), ('교통', 0.9972415566444397), ('접근성', 0.9972261190414429),
    ('보', 0.9972063899040222), ('방도', 0.9972002506256104)]
    리뷰 데이터 : None
========================================================================================================================
* 데이터가 올바르게 출력되었다. 하지만 word2vec에 대한 이해도가 낮은 상태에서 코드만 넣어서 확인한것이기 때문에
word2vec 공부를 해야함.






# 기록 7 ( 훈련된 모델 shape 확인 )
========================================================================================================================
okt 형태소 분류...
100%|██████████████████████████████████████| 3001/3001 [00:14<00:00, 200.77it/s]
Vectorizing sequence data...
(2065, 100)
========================================================================================================================


# 기록 8 ( 리뷰 통계 : image dir에서 png 확인)
=======================================================================================================================
리뷰의 최대 길이 : 150
리뷰의 평균 길이 : 25.571476174608463
=======================================================================================================================



# 기록 9 ( "만족" 과 비슷한 단어 찾기)
=======================================================================================================================
* sg = 0, min_count = 5
[('많이', 0.9998157024383545), ('정도', 0.9998121857643127), ('사람', 0.9998056888580322), ('식사', 0.9998053312301636),
('된', 0.9998018145561218), ('온도', 0.9998000860214233), ('없고', 0.9997991323471069), ('분위기', 0.9997941851615906),
 ('깨끗한', 0.9997904300689697), ('되는', 0.9997903108596802)]

* sg = 0, min_count = 10
Vectorizing sequence data...
[('도', 0.9998130798339844), ('같은', 0.9998012781143188), ('건', 0.9997950792312622), ('수영장', 0.9997948408126831),
('않아', 0.999783456325531), ('음식', 0.9997806549072266), ('분위기', 0.9997782707214355), ('신경', 0.9997768402099609),
('굉장히', 0.9997766017913818), ('해서', 0.9997742772102356)]

* sg = 0, min_count = 15
[('하지만', 0.999809741973877), ('서', 0.9997804164886475), ('와인', 0.9997660517692566), ('이었습니다', 0.9997588992118835),
('느낌', 0.9997587203979492), ('사람', 0.9997577667236328), ('인테리어', 0.9997565746307373), ('소음', 0.9997557401657104),
('아이', 0.9997552633285522), ('내', 0.9997514486312866)]

* sg = 0, min_count = 20
[('분위기', 0.9997180700302124), ('깨끗한', 0.9997081160545349), ('만족스러웠습니다', 0.9996935129165649), ('깔끔한', 0.9996843338012695),
('와인', 0.9996829628944397), ('큰', 0.9996799230575562), ('면', 0.9996751546859741), ('배려', 0.9996702075004578),
('않지만', 0.9996699094772339), ('않은', 0.9996668100357056)]

* sg = 0, min_count = 25
[('와인', 0.9997572302818298), ('보고', 0.9997364282608032), ('이었습니다', 0.9997293949127197), ('맥주', 0.9997281432151794),
('굉장히', 0.9997214078903198), ('코로나', 0.9997187852859497), ('식사', 0.9997166395187378), ('걸', 0.9997084736824036),
('좋아', 0.9997069239616394), ('문제', 0.9997051954269409)]

* sg = 0, min_count = 30
[('전혀', 0.9997217655181885), ('식사', 0.9997193217277527), ('만족스러웠습니다', 0.9997138977050781), ('였습니다', 0.999710202217102),
('방이', 0.9996978044509888), ('딱', 0.9996961355209351), ('굉장히', 0.9996952414512634), ('않은', 0.9996923208236694),
('볼', 0.9996908903121948), ('제공', 0.9996904730796814)]

************* sg 값 변경 ************************

* sg = 1, min_count = 5
[('친절했고', 0.9958021640777588), ('물론', 0.9955589771270752), ('끔', 0.9953944683074951), ('깔', 0.994544267654419),
('나', 0.9944262504577637), ('친절하시니', 0.9941003918647766), ('친절하세요', 0.9938473105430603), ('만족했습니다', 0.9937354922294617),
('너무나', 0.993686854839325), ('훌륭한', 0.993354856967926)]

* sg = 1, min_count = 10
[('위생', 0.995941162109375), ('깨끗했고', 0.9959219098091125), ('쾌적한', 0.9956857562065125), ('심플', 0.9951194524765015),
('좋았음', 0.9949259757995605), ('훌륭하고', 0.993772029876709), ('고급스러운', 0.993500828742981), ('깨끗해', 0.9931552410125732),
('나', 0.9931240081787109), ('만족스럽습니다', 0.9929053783416748)]

* sg = 1, min_count = 15
[('쾌적한', 0.9949052333831787), ('물론', 0.9939712882041931), ('위생', 0.9927648305892944), ('편한', 0.9920718669891357),
('좋았음', 0.9917362928390503), ('끔', 0.9915804862976074), ('고', 0.9909809827804565), ('친절', 0.9909777641296387),
('고급스러운', 0.9908297061920166), ('정비', 0.9907188415527344)]

* sg = 1, min_count = 20
[('만족스럽습니다', 0.9956712126731873), ('나', 0.9946420192718506), ('쾌적한', 0.9943250417709351), ('편한', 0.9942020773887634),
('물론', 0.9931555390357971), ('보니', 0.9931144118309021), ('고', 0.9930259585380554), ('모던', 0.9926996231079102),
('자체', 0.9923812747001648), ('했고', 0.9920921325683594)]

* sg = 1, min_count = 25
[('보니', 0.9961726665496826), ('고', 0.9959455728530884), ('편', 0.9952077865600586), ('물론', 0.9934786558151245),
('딱', 0.9934673309326172), ('이었는데', 0.9930583834648132), ('않은', 0.9926679134368896), ('도시락', 0.9926109313964844),
('되지', 0.992536723613739), ('되고', 0.9915354251861572)]

* sg = 1, min_count = 30
[('모던', 0.994388222694397), ('대', 0.9939221143722534), ('않은', 0.9937033653259277), ('나', 0.9933357834815979),
('고', 0.9929693937301636), ('그럭저럭', 0.9909741878509521), ('가지', 0.9906245470046997), ('인상', 0.9905768632888794),
('이었는데', 0.9903640151023865), ('이상', 0.9902962446212769)]
========================================================================================================================
* sg = 0 인 경우 min_count 수치를 높을수록 유사성이 높았다.
* sg = 1 인 경우 min_count 수치가 높을수록 유사성이 낮았다.
min_count 수치가 높을수록 학습할수 있는 단어의 개수가 줄어들기 때문에 min_count 수치가 낮을수록 효율이 좋은 sg = 1 값을
사용하는것이 적절하다.





# 기록 10
* okt 품사 태그 확인하여 다양하게 적용해보기
========================================================================================================================
(실행코드)
    print(okt.tagset)

(결과)
    {'Adjective': '형용사', 'Adverb': '부사', 'Alpha': '알파벳', 'Conjunction': '접속사', 'Determiner': '관형사',
    'Eomi': '어미', 'Exclamation': '감탄사', 'Foreign': '외국어, 한자 및 기타기호', 'Hashtag': '트위터 해쉬태그',
    'Josa': '조사', 'KoreanParticle': '(ex: ㅋㅋ)', 'Noun': '명사', 'Number': '숫자', 'PreEomi': '선어말어미',
    'Punctuation': '구두점', 'ScreenName': '트위터 아이디', 'Suffix': '접미사', 'Unknown': '미등록어', 'Verb': '동사'}

* 필요한 품사 : 형용사, 동사, 명사, 부사, 감탄사
                , 관형사(명사의 상황을 나타내줌(새 것, 헌 것)), 미등록어(감정표현이 들어가있는 경우가 있음)
 in {'Adjective', 'Verb', 'Noun', 'Adverb', 'Exclamation', 'Determiner', 'Unknown'}

* 삭제대상 품사 : 접속사, 알파벳, 외국어, 해시태그, 조사(을를이가), ㅋㅋ, 숫자, 구두점,
                  접미사(명사를 꾸며주기 때문에 필요? 떨어져 나온것은 의미가 없다)
                  선어말어미(이미 떨어져나온 선어말 어미라면 의미를 갖기 힘들다), 어미
not in {'Alpha', 'Conjunction', 'Eomi', 'Foreign', 'Hashtag', 'Josa', 'Number', 'KoreanParticle', 'PreEomi', 'Punctuation', 'Suffix'}
========================================================================================================================



# 기록 11
========================================================================================================================
* 기존 품사 분류
    if word[1] not in ['Josa', 'Eomi', 'Punctuation', 'Suffix']:

    okt 품사적용 테스트 :
    [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '사', '통', '팔', '달', '교통', '편리'],
    ['강력', '추천'], ['매우', '나쁨'], ['구', '성비', '갑', '최고', '호텔'], ['좋음'], ['최고'], ['혼캉스', '딱', '좋아요'], ['강력', '추천'],
    ['최고'], ['최고'], ['형편', '없음'], ['가성', '비각', '맞네요'], ['강력', '추천'], ['강력', '추천'], ['매우', '좋음'], ['만족'], ['매우', '좋음'],
    ['만족'], ['강력', '추천'], ['매우', '좋음'], ['서울', '최', '새', '호텔'], ['만족'], ['매우', '좋음'], ['매우', '좋음'], ['매우', '좋음']]


* 수정된 품사 분류
# 감정을 표현하는데 적합한 품사만 가져옴

    if word[1] in ['Adjective', 'Verb', 'Noun', 'Adverb', 'Exclamation', 'Determiner', 'Unknown']:
        # 감정을 표현하는 품사만 따로 가져 오는 것이 유용하다고 판단
          - 어떤 데이터가 있는지 알기 쉽고, 필요에 따라 추가시키기도 편하다.

        # 'Adjective', 'Verb', 'Noun', 'Adverb' : 형용사, 동사, 명사, 부사는 문장 구성 기본이라 필수로 구성.
        # Exclamation : 감탄사는 감정의 가장 단순한 형태여서 꼭 필요하다고 생각
        # Determiner : 관형사는 명사를 꾸며주기 때문에 중요하다고 판단.
        # Unknown : 미등록어는 널리쓰이는 유행어 같은 것을 놓치지 않기 위해 선택
        예시) 어떤 아이가 책을 열심히 읽는다.
        어떤: 관형사, 아이: 명사, 열심히: 부사, 읽는다: 동사
        * skip-gram 형식을 사용하기 때문에 단어의 앞뒤에 밀접하게 붙는 품사가 중요하다.

    okt 품사적용 테스트 :
    [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '통', '달', '교통', '편리'],
    ['강력', '추천'], ['매우', '나쁨'], ['성비', '갑', '최고', '호텔'], ['좋음'], ['최고'], ['혼캉스', '딱', '좋아요'], ['강력', '추천'],
    ['최고'], ['최고'], ['형편', '없음'], ['가성', '비각', '맞네요'], ['강력', '추천'], ['강력', '추천'], ['매우', '좋음'], ['만족'], ['매우', '좋음'],
    ['만족'], ['강력', '추천'], ['매우', '좋음'], ['서울', '최', '새', '호텔'], ['만족'], ['매우', '좋음'], ['매우', '좋음'], ['매우', '좋음']]


    기존 : ['서울', '내', '어디', '사', '통', '팔', '달', '교통', '편리'],
    수정 : ['서울', '내', '어디', '통', '달', '교통', '편리'],
    * 수정된 품사 분류가 필요한 데이터만 가져오는데 더 효과적



* 어근화 적용( 얻는것에 비해 잃는게 많다)
     ok = okt.pos(i, stem=True)
     * 편안하다, 조용하다 같은 동일한 형태로 데이터를 얻지만, word2vec의 경우 군집화를 통해 어근이 같은 문장은 같은 클러스터에
     모이게 되기 때문에 학습데이터가 많아지면 '편해서' 와 '편안하고' 가 굉장히 가까운 위치에 있을 것이기 때문에 어근화를 진행하지 않아도
     된다고 판단했다. 어근화를 진행할경우 학습 데이터의 종류가 적어지기 때문에 미래에 모델이 대응할수 있는 리뷰의 종류도 제한된다고 판단.

    [['형편', '없다'], ['가성', '비각', '맞다'], ['강력', '추천'], ['강력', '추천'], ['매우', '좋다'], ['만족'], ['매우', '좋다'],
    ['만족'], ['강력', '추천'], ['매우', '좋다'], ['서울', '최', '새', '호텔'], ['만족'], ['매우', '좋다'], ['매우', '좋다'], ['매우', '좋다'],
    ['좋다', '다만', '주', '차비', '내다', '거', '이해'], ['좋다', '시간', '보내다', '또', '방문', '싶다'], ['주말', '휴가'], ['강력', '추천'],
     ['강력', '추천'], ['최고'], ['강력', '추천'], ['좋다'], ['실망', '스럽다', '움'], ['매우', '좋다'], ['강력', '추천'], ['매우', '좋다'], ['강력', '추천'],
     ['매우', '좋다'], ['최고'], ['최고'], ['좋다'], ['좋다'], ['조용하다', '편안하다', '휴식'], ['역시', '가든', '호텔'], ['단', '채', '오다', '때', '아침', '어떻다', '합'],
      ['강력', '추천'], ['좋다'], ['최고'], ['만족'], ['강력', '추천'], ['최고'], ['강력', '추천'], ['실망', '스럽다', '움'], ['좋다'], ['최고'],
      ['편리하다', '위치', '누구', '알다', '유명하다'], ['코로나', '이전', '정말', '좋다', '숙소', '이다'], ['매우', '좋다'], ['그럭저럭'], ['최고'],
      ['침대', '밑', '위', '머리카락', '더러', '움'], ['그럭저럭'], ['항상', '편안하다', '숙소'], ['만족'], ['청결하다', '건물', '노후', '다르다'],
      ['매우', '좋다'], ['예전', '조금', '만족', '않다', '하루'], ['오랜', '만', '서울', '오다', '푹', '쉬다'],
      ['기본', '룸', '좀', '더', '비싸다', '가격', '이다', '테라스', '룸', '층', '있다', '고', '층', '원하다', '분', '테라스', '룸비', '추설', '명', '다르다', '안', '돼다', '있다'],
      ['로비', '압도', '당하다', '규모', '역시', '명불허전', '베스트', '웨스턴', '가든', '호텔', '제', '배정', '받다', '곳', '층뷰', '좋다', '모든', '것', '특급', '호텔', '느끼다',
      '충분하다', '곳', '이다', '어느', '것', '하나', '준비', '것', '느껴지다', '새롭다', '생기다', '호텔', '비교', '불가', '이다', '가든', '호텔', '최고', '호텔', '이다'],
      ['주말', '데이트', '찾아가다', '서비스', '좋다', '호텔', '보다', '야경', '좋다', '다', '만', '지하철역', '조금', '떨어지다', '있다', '걸다', '되다'],
      ['작년', '처음', '방문', '후', '휴식', '및', '작업', '차', '다시', '방문', '하다', '재', '방문', '되다', '이유', '위치', '교통', '편', '편리', '버스',




# 기록 12
* word2vec 모델 프린트
============================================================
Vectorizing sequence data...
모델 shape : (2048, 100)
모델 프린트 : Word2Vec(vocab=2048, size=100, alpha=0.025)
============================================================


# 기록 13
* okt 형태소 분석 자료 csv 저장
========================================================================================================================
(실행코드)
def okt_csv(file_name):

    df_all = csv_reader(file_name)  # csv 파일 load
    df_preprocess = df_all.loc[:, ['review_id', 'score', 'review', 'preprocessed_review']]  # 점수, 전처리된 리뷰, 리뷰id(merge할때 필요)
    df_clean = df_preprocess.dropna(axis=0)  # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True)  # 인덱스 재정렬

    x, y = df_reindex['preprocessed_review'], df_reindex['score']  # x 리뷰, y 점수

    print('okt 형태소 분류...')  # 형태소 분류
    df_x1 = pd.DataFrame(x)  # 리뷰 데이터 프레임 변환
    tokenized_riviews = okt_morph(df_x1)  # 전처리된 리뷰 데이터 토크나이징

    dict_okt = {'okt_pos_review': tokenized_riviews}
    df_okt = pd.DataFrame(dict_okt)
    df_merge = pd.merge(df_reindex, df_okt, right_index=True, left_index=True)
    df_merge.to_csv(f"./data/{file_name}_Okt_version.csv", encoding='utf-8')

(csv 형태)
    ,review_id,score,review,preprocessed_review,okt_pos_review
    0,4561805431,4,"""좋음""",좋음,['좋음']
    1,2895377478,4,"""좋음""",좋음,['좋음']
    2,3397527294,5,"""전체적으로 만족도 높음""",전체적으로 만족도 높음,"['전체', '만족도', '높음']"
    3,2687126186,2,"""그럭저럭""",그럭저럭,['그럭저럭']
    4,2779753255,4,"""최고""",최고,['최고']
========================================================================================================================
* 나중에 원본 파일에 join 하기 위해 review_id 컬럼을 함께 가져옴

# 기록 14
==============================
okt 형태소 분류...
100%|██████████| 150552/150552 [05:14<00:00, 478.81it/s]
==================================
* 네이버 리뷰 형태소 분류까지 완료.



# 기록 15
* imdb 데이터셋확인
=================================================
x_train shape: (50000, 1000)
(array([[0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.]]), array([1, 0, 0, ..., 0, 0, 0]))

==================================================


# 기록 16
* 형태소 분석 리뷰 data 벡터화 과정
===================================================
* 리뷰 데이터 형태 2차원 리스트
    ["['좋음']", "['좋음']", "['전체', '만족도', '높음']", "['그럭저럭']", "['최고']", "['서울', '내', '어디', '통', '달', '교통', '편리']", "['강력', '추천']", "['매우', '나쁨']", "['성비', '갑', '최고', '호텔']", "['좋음']", "['최고']"]
* 한개만 출력시
    ['좋음']

* 라벨링 데이터 형태
    [4, 4, 5, 2, 4, 5, 5, 1, 5, 4, 4]
===================================================


# 기록 17
* 모델 세이브
* 전체 리뷰 600만개
===================================================
ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /home/ubuntu/itbiz/user3/DEC/datasets.py
2021-09-27 18:26:43.168251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Loading data...

/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583:
* 경고가 뜬채로 진행이 안되고 있다.
* 파이썬과 넘파이의 충돌이 발생할수 있음을 경고
FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
  종료 코드 -1(으)로 완료된 프로세스
===================================================
* 모델파일은 저장성공하였다.



# 기록 18
* 데이터 벡터화 시도
==================================================================
5 classes
Traceback (most recent call last):
  File "/home/ubuntu/itbiz/user3/DEC/datasets.py", line 502, in <module>
    x, y = load_crawleing_reviews('all_hotels_review_data(final)', model_20)
  File "/home/ubuntu/itbiz/user3/DEC/datasets.py", line 408, in load_crawleing_reviews
    idx_to_key = model.wv.index_to_key
AttributeError: 'Word2VecKeyedVectors' object has no attribute 'index_to_key'

종료 코드 1(으)로 완료된 프로세스
=================================================================
* gpu에서는 index_to_key attribute가 없다.(옛날버전?)



# 기록 19
* 리뷰 데이터 10만개로 word2vec 모델 만들어서 3d 재현 시도(도현쓰)
============================================================================
ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /home/ubuntu/itbiz/user3/DEC/datasets.py
2021-09-28 14:08:05.899418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
100%|█████████████████████████████████| 809737/809737 [04:22<00:00, 3085.37it/s]
==============================================================================
* 모델 save시 입력데이터의 형태에 신경써야 한다
(전체 list 안쪽에 각각의 list, 각각의 list 안에 문자열로 데이터가 들어가있어야함)


# 기록 20
* word2vec 입력 데이터(리스트) 피클파일 형식으로 저장하기
================================================================================
* 사용한 코드
    # 리스트 파일로 저장(피클)
    def save_list(list):
        with gzip.open('review_list.pkl', 'wb') as f:
            pickle.dump(list, f, pickle.HIGHEST_PROTOCOL)

    # 리스트 파일 불러오기(피클)
    def load_list():
        with gzip.open('review_list.pkl', 'rb') as f:
            data = pickle.load(f)
        return data

* 리뷰 10개 테스트 결과
(저장, 로드 모두 정상 작동 확인)
    100%|████████████████████████████████████████| 10/10 [00:00<00:00, 72691.58it/s]
    [['생각', '더러워서', '놀랐습니다'], ['관리', '상태', '양호', '해', '보이진', '않았습니다', '로열', '스위트', '객실'], ['화장실', '문', '안', '닫', '김', '침대', '두', '개', '꾸역꾸역', '들어가', '있어서', '세면대', '세게', '틀', '가까운', '침대', '물이', '튐'], ['지은지', '얼마', '안', '된', '건물', '알', '있는데', '객실', '내', '시설', '상당히', '노후', '화', '관리', '안', '되고', '있는', '상태', '였습니다', '욕조', '침실', '구분', '없어', '엄청', '습하였고', '여기저기', '곰팡이', '보였습니다'], ['변기', '잘', '안', '내려가는', '거', '빼곤', '다', '좋았어요'], ['욕조', '좋아해서', '호텔', '더', '디자이너', '스', '호텔', '체인', '가끔', '이용', '하는데', '이제', '이용', '해본', '더', '디자이너', '스', '호텔', '중', '최악', '이었어요', '서울역', '삼성역', '여기', '좀', '관리', '상태', '별로', '였지만', '군자역', '신', '논현', '점', '다', '이용', '해', '봤는데', '이렇게', '이', '용이', '기분', '나빴을', '정도', '관리', '안', '된', '곳', '홍대', '점', '처음', '요', '관리', '정말', '안', '되어', '있고', '이', '돈', '이', '그냥', '욕조', '있는', '모텔', '낫다', '싶은', '수준', '욕조', '더럽고', '바닥', '부품', '빠져', '있어서', '살', '긁', '힐', '뻔했어요', '수전', '덜컹', '거리', '상태', '최악', '화장실', '문도', '제대로', '안', '닫', '겨', '샤워실', '불', '잘', '안', '들어옴', '리모컨', '하면', '들어옴', '호텔', '자주', '이용', '하는데', '이런', '후기', '남기는', '거', '정말', '처음', '너무', '기분', '나쁜', '주말', '이었어요', '더', '디자이너', '스', '호텔', '다시', '예전', '같은', '수준', '돌아왔으면', '좋겠네요'], ['너무', '추웠는데', '바닥', '미리', '따뜻하게', '데워졌었고요', '욕조', '널찍하면서도', '디자인', '심플', '하게', '예뻤어요', '층', '이었는데', '야경', '꽤', '괜찮았고', '호텔', '바로', '앞', '편의점', '있어서', '편했고요', '조용하고', '이불', '침대', '너무', '포근했네요'], ['호텔', '조용하고', '깔끔했습니다', '직원', '들', '친절하여', '기분', '좋게', '이용', '했습니다', '다음', '또', '방문', '싶습니다'], ['좋긴', '좋은데', '와인', '컵', '제공', '해달라니까', '룸서비스', '이용', '시', '가능하다고', '해서', '종이컵', '먹었어요', '크리스마스', '시즌', '평소', '숙박', '비보', '비싸게', '주고', '잤는데', '흠', '조식', '제공', '되는', '샌드위치', '맛있습니다'], ['무슨', '탕', '비실', '옆방', '준', '건지', '진심', '그렇게', '방음', '안', '되는', '방', '처음', '이었어요', '시끄러워', '요', '새벽', '내내', '잠들', '기', '어려웠어요']]

    종료 코드 0(으)로 완료된 프로세스


* 리뷰 전체 리스트로 저장
100%|███████████████████████████████| 5941314/5941314 [37:20<00:00, 2651.42it/s]

'review_list.pkl'로 저장완료
======================================================================
* 저장한후 load_list() 로 0번째 데이터 형태 확인
* 사용한 코드
    test_list = load_list()
    print(test_list[0])
    print(type(test_list[0]))
* 결과
    ['생각', '더러워서', '놀랐습니다']
    <class 'list'>




# 기록 21
* 클러스터링 결과 출력해봄
===============================================================================
{'리뷰': array([[-4.45704073e-01, -3.66882801e-01,  1.31100342e-01, ...,
        -1.49253443e-01,  2.40333900e-01,  1.52797103e-01],
       [-1.86707556e-01, -2.63934553e-01,  1.05644889e-01, ...,
        -1.12249106e-01,  1.57335445e-01,  1.78158164e-01],
       [-1.15661725e-01,  1.69973262e-03,  8.60931575e-02, ...,
        -4.40693237e-02, -7.90716782e-02, -2.61354586e-03],
       ...,
       [-5.21343667e-04, -1.70271465e-04,  3.80239915e-04, ...,
        -1.88938269e-04,  2.01341463e-05,  1.40015822e-04],
       [-2.01178715e-04, -1.81344498e-04,  1.71244261e-04, ...,
        -1.09002882e-04,  6.75640986e-05,  7.22189434e-05],
       [-1.20891120e-04, -2.69130687e-04,  1.49364176e-04, ...,
         2.45131505e-05,  1.09895715e-04,  8.45251270e-05]]), '라벨링': array([3, 3, 2, 1, 5, 1, 5, 5, 5, 2, 1, 5, 4, 4, 4, 4, 3, 5, 4, 2, 3, 4,
       4, 3, 1, 3, 3, 4, 4, 5, 4, 3, 4, 3, 3, 3, 4, 2, 5, 4, 2, 3, 1, 5,
       1, 4, 2, 2, 4, 4, 1, 5, 3, 3, 5, 4, 5, 3, 5, 3, 5, 4, 1, 1, 4, 2,
       1, 2, 5, 1, 3, 4, 5, 5, 3, 3, 1, 4, 1, 5, 2, 3, 4, 5, 3, 4, 3, 5,
       5, 5, 5, 2, 3, 4, 4, 5, 5, 5, 3, 5, 2, 5, 5, 5, 4, 3, 5, 4, 5, 3,
       1, 1, 1, 5, 5, 2, 1, 5, 4, 4, 2, 2, 4, 4, 5, 5, 5, 2, 4, 4, 2, 5,
       4, 4, 4, 5, 5, 4, 3, 4, 2, 2, 3, 4, 4, 4, 4, 5, 2, 4, 4, 5, 5, 4,
       4, 5, 5, 4, 5, 1, 5, 5, 4, 4, 5, 4, 5, 3, 3, 4, 4, 5, 5, 5, 5, 4,
       4, 3, 5, 5, 3, 4, 5, 4, 4, 2, 4, 3, 4, 5, 5, 5, 4, 4, 4, 4, 2, 4,
       4, 5, 4, 3, 5, 5, 3, 2, 1, 4, 5, 5, 1, 5, 5, 5, 2, 4, 3, 5, 4, 5,
       4, 5, 4, 5, 3, 5, 1, 4, 2, 4, 2, 4, 5, 5, 5, 4, 3, 4, 5, 5, 5, 5,
       3, 4, 5, 5, 3, 5, 3, 4, 2, 5, 3, 5, 4, 5, 4, 3, 5, 5, 5, 3, 5, 4,
       2, 5, 5, 5, 4, 4, 4, 4, 5, 5, 2, 5, 4, 3, 3, 5, 4, 5, 2, 5, 4, 5,
       3, 3, 4, 4, 5, 4, 5, 3, 2, 3, 5, 5, 5, 5, 3, 1, 5, 3, 4, 5, 5, 4,
       3, 3, 4, 4, 5, 5, 2, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5,
       5, 1, 5, 5, 5, 5, 3, 4, 4, 5, 5, 1, 4, 5, 4, 5, 5, 4, 5, 3, 3, 5,
       5, 4, 5, 1, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 3, 5, 4, 5, 3, 5, 5, 5,
       4, 5, 3, 4, 5, 5, 5, 5, 5, 3, 4, 3, 4, 4, 4, 5, 2, 3, 5, 4, 2, 4,
       1, 3, 5, 5, 3, 5, 5, 5, 5, 1, 4, 4, 4, 5, 5, 3, 5, 4, 5, 5, 2, 3,
       4, 3, 1, 3, 4, 5, 5, 5, 5, 4, 4, 5, 5, 3, 5, 3, 1, 5, 5, 3, 3, 3,
       3, 5, 5, 4, 3, 2, 5, 2]), '예측값': array([4, 1, 2, 2, 3, 2, 3, 3, 3, 3, 3, 0, 0, 0, 3, 0, 3, 0, 3, 0, 0, 0,
       0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0])}

종료 코드 0(으)로 완료된 프로세스
=====================================================================================================
* 리뷰가 벡터형태로 나오기 때문에 텍스트로 변환한뒤에 csv 저장을 시도해볼 예정



# 기록 22
* 1000개의 리뷰로 test 진행
  (벡터, 리뷰, 점수, 클러스터넘버)
=====================================================================================================
1. csv 파일로 저장하는데 성공
2. 전체 데이터를 csv로 저장하여 차후에 벡터화하는 시간을 절약할수 있다.
=====================================================================================================



# 기록 23
==================================================================================
* 50만개 test

* min_cnt : 10
* 정확도 : 0.7113321708945689

* min_cnt : 20
* 정확도 : 0.7106447319213277

* min_cnt : 30
* 정확도 : 0.7112969364308394

* min_cnt : 40
* 정확도 : 0.7106962940292918
==================================================================================
* 입력데이터 수정후에 정확도가 올랐다!!


# 기록 24
* 클러스터링 문제 발생??
* 클러스터링이 지나치게 편향적이다.
* 육안으로 확인시 부정적인 리뷰가 긍정 리뷰와 함께 있는것 다수 확인
=====================================================
* min_cnt 10 일떄
    각 스코어 개수
    5    354102
    4     92816
    3     30359
    2     12631
    1      7896
    Name: score, dtype: int64
    각 클러스터 개수
    4    497798
    2         2
    3         2
    0         1
    1         1
    Name: cluster_num, dtype: int64

* min_cnt 30일때
    각 스코어 개수
    5    353521
    4     92694
    3     30292
    2     12615
    1      7880
    Name: score, dtype: int64
    각 클러스터 개수
    0    496979
    3        18
    1         3
    2         1
    4         1
    Name: cluster_num, dtype: int64
=======================================================


# 기록 25
* cluster별로 csv 저장하여 리뷰를 살펴 보았다.
==============================================================
* 사용한 코드

    # 클러스터 num 으로 구분하여 csv 저장후 자료 살펴보기
    df_cluster_all = csv_reader('test_cluster_data_min_cnt_30')
    cluster_num = 5
    for num in range(cluster_num):
        cluster_condition = (df_cluster_all.cluster_num == num)
        df_cluster_each = df_cluster_all[cluster_condition]
        csv_save(df_cluster_each, f'cluster_{num}')

    # 클러스터링 통계 구해 보기
    cluster_num = 5
    for num in range(cluster_num):
        df_test_by_cluster_num = csv_reader(f'cluster_{num}')
        print(f'** cluster_num : {num} **')
        print('** score 분포 **')
        print('==================')
        print(df_test_by_cluster_num['score'].value_counts())
        print('==================')


* 결과

** cluster_num : 0 **
** score 분포 **
==================
5    353509
4     92687
3     30290
2     12614
1      7879
Name: score, dtype: int64
==================


** cluster_num : 1 **
** score 분포 **
==================
4    1
5    1
2    1
Name: score, dtype: int64
==================

** cluster_num : 2 **
** score 분포 **
==================
3    1
Name: score, dtype: int64
==================

** cluster_num : 3 **
** score 분포 **
==================
5    11
4     5
1     1
3     1
Name: score, dtype: int64
==================

** cluster_num : 4 **
** score 분포 **
==================
4    1
Name: score, dtype: int64
==================

종료 코드 0(으)로 완료된 프로세스
=============================================================================================
* cluster 0 번으로 모든 점수의 리뷰가 많이 들어가있다.
* 애초에 5점이 가장 많기 때문에 이런 결과가 생긴 것일까?
* 600만개의 리뷰 데이터에서 score 항목의 분포를 알아보기로 했다.





# 기록 26
* 모든 리뷰 데이터 'total_score' 분포
===============================================================

모든 리뷰 데이터 score 분포(600만개)
5	4321290	0.704 100
4	1179579	0.192 50
3	381660	0.062
2	155983	0.025
1	97404	0.015
sum	6135916
*******************************
테스트 리뷰 데이터 score 분포(50만개)
5	354102	0.711
4	92816	0.186
3	30359	0.06
2	12631	0.025
1	7896	0.015
sum	497804
********************************
테스트 리뷰 클러스터 분포
4	497798	0.999
2	2	    0
3	2	    0
0	1	    0
1	1	    0
sum	497804
=============================================================
* 전체 리뷰의 분포와 random 추출한 50개의 test_data의 비율은 일정하다
* 4점이 약 18~19% 인것에 비해 클러스터링이 너무 부족하게 되었다.
* 실제 리뷰를 읽어보더라도, 5점으로 분류된것 같은 곳에 부정적인 의견의 리뷰가
    다수 발견 되었다.

< 수정된 클러스터링 계획 >
1. 원본 csv 데이터에서 점수별로 나눈다.
2. 각각의 csv 데이터를 이진 분류 클러스터링을 수행, 순수한 데이터를 얻는다.
3. 클러스터링된 csv를 merge하여 다시 하나의 데이터로 만든다.


# 기록 27
* 3점 data 이진 클러스터링
===============================================================

3 classes
Vectorizing sequence data...
벡터화: 100%|█████████████████████████| 363926/363926 [3:39:18<00:00, 27.66it/s]
361134 train sequences
x_train shape: (361134, 100)
y shape: (361134,)
raw_reviews shape: 361134


Pretraining time: 431s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 2)                 4
=================================================================
Total params: 1,307,006
Trainable params: 1,307,006
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 7050
Initializing cluster centers with k-means.

acc: 0.9999916928342388
clustering time:  44.54625368118286


* 클러스터링 결과

    3    361134
    Name: score, dtype: int64
    각 클러스터 개수
    0    361131
    1         3
=================================================================================
* 클러스터링 된 데이터의 개수가 작다.
* 리뷰간 차이점을 알기 어려웠다.
* 분류를 어떻게 해야할지 굉장히 난감한 상황.

* 5개 클러스터는 군집화 결과가 너무 좋지 않았고, 이진분류를 해본뒤 전체적인 판단을 해볼예정
* 5점짜리 리뷰는 너무 많아서 5번을 제외하고 1~4점까지의 리뷰데이터를 이진분류 시도



# 기록 28
* 2점 리뷰
===========================================================================================
2 classes
Vectorizing sequence data...
벡터화: 100%|███████████████████████████| 149639/149639 [40:39<00:00, 61.35it/s]
148875 train sequences
x_train shape: (148875, 100)
y shape: (148875,)
raw_reviews shape: 148875

Pretraining time: 199s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 2)                 4
=================================================================
Total params: 1,307,006
Trainable params: 1,307,006
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 2905
Initializing cluster centers with k-means.

acc: 0.9999865659109992
clustering time:  19.647976875305176


* 클러스터링 결과
    각 스코어 개수
    2    148875
    Name: score, dtype: int64
    각 클러스터 개수
    0    148873
    1         2
Name: cluster_num, dtype: int64

======================================================
* 역시나 별로 .. 효과가 그닥이다.




# 기록 29
* 극단적인 test data를 만들어서 클러스터링 test 해보기
* 1점짜리 5점짜리 각 1000개씩 총 2000개의 데이터를 1로 재라벨링해서 테스트
    * 데이터는 최소한 256보다 커야한다. 안그러면 zerodivision에러 난다.
========================================================================
1 classes
Vectorizing sequence data...
벡터화: 100%|████████████████████████████| 2000/2000 [00:00<00:00, 10119.33it/s]
1865 train sequences
x_train shape: (1865, 100)
y shape: (1865,)
raw_reviews shape: 1865

Pretraining time: 6s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 2)                 4
=================================================================
Total params: 1,307,006
Trainable params: 1,307,006
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 35

acc: 0.9935656836461126
clustering time:  2.275355577468872

종료 코드 0(으)로 완료된 프로세스


* 클러스터 결과 확인
    각 스코어 개수
1    1865
Name: total_score, dtype: int64
각 클러스터 개수
0    1853
1      12
Name: cluster_num, dtype: int64
=================================================
* 1점과 5점을 반반씩 섞어도 클러스터링이 잘 되지 않았다.
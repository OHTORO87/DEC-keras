# 기록 1
* GPU 할당 번호를 바꿈
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
* mnist 테스트
========================================================================================================================
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 784)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               392500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 10)                20010
_________________________________________________________________
clustering (ClusteringLayer) (None, 10)                100
=================================================================
Total params: 1,665,110
Trainable params: 1,665,110
Non-trainable params: 0
_________________________________________________________________
Update interval 140
Save interval 1365
Initializing cluster centers with k-means.


Iter 5880: acc = 0.96059, nmi = 0.90227, ari = 0.91529  ; loss= 0.11461
delta_label  0.0009 < tol  0.001
Reached tolerance threshold. Stopping training.
saving model to: results/DEC_model_final.h5
/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/sklearn/utils/linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.
  warnings.warn(
acc: 0.9605857142857143
clustering time:  180.99016213417053
========================================================================================================================



# 기록 2 ( score 편집 )
========================================================================================================================
* 네이버 score 편집
    2로 나누고 > round > 0점인 경우 1로 바꿈 >int
* 사용한 코드

    # 데이터 부르기
    df_naver = csv_reader("naver_reiview_preprocess_latest_merge")

    # 모든값을 반으로 나눈후 반올림
    df_naver['score'] = round(df_naver['score'] / 2)

    # 0값이 있는 경우 1로 변경
    df_naver.loc[df_naver['score'] == 0, 'score'] = 1

    # score float > int 변경
    df_dtype_int = df_naver.astype({'score': int})

    # 저장
    csv_save(df_dtype_int, 'naver_reiview_preprocess_latest_merge')
========================================================================================================================
* 1점을 2로 나눈후 round(0.5) == 0 현상 발생 > 0점을 1점으로 변경




# 기록 3
=======================================================================================================================
* 사용한 코드
    df_all = csv_reader(file_name) # csv 파일 load
    df_preprocess = df_all.loc[:, ['score','preprocessed_review']] # 점수와 전처리된 리뷰만 가져옴
    df_clean = df_preprocess.dropna(axis=0) # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True) # 인덱스 재정렬

* 결과
    ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /tmp/pycharm_project_367/DEC-keras/datasets.py
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 3001 entries, 0 to 3000
    Data columns (total 2 columns):
     #   Column               Non-Null Count  Dtype
    ---  ------               --------------  -----
     0   score                3001 non-null   int64
     1   preprocessed_review  3001 non-null   object
    dtypes: int64(1), object(1)
    memory usage: 47.0+ KB
          score                                preprocessed_review
    0         4                                                 좋음
    1         4                                                 좋음
    2         5                                       전체적으로 만족도 높음
    3         2                                               그럭저럭
    4         4                                                 최고
    ...     ...                                                ...
    2996      2  하루 묵기는 무난합니다 중앙난방인 지방에 냉방을 요청했는데 난방 시즌이어서 냉방은 ...
    2997      2  박일 숙박했는데 너무 깔끔하고 전망도 아주 좋았습니다 층 디럭스룸 배정받았는데 웬만...
    2998      2  편히 쉬다 가기 좋은 곳 위치도 좋고 무엇보다 침구류가 좋아서 편안하게 휴식 취할 ...
    2999      2  멤버십 가입 후 처음 이용했는데 정말 퇴실하기 싫을 정도로 좋았습니다 분이 친절하게...
    3000      2  한국 갈 때마다 롯데시티나 신라스테이를 이용했는데 왜 글래드를 몰랐는지 아쉽다 앞으...

    [3001 rows x 2 columns]
    None
=======================================================================================================================
* 5000개의 리뷰 데이터가 있는 csv 파일에서 nan 값을 제외한 3000개의 데이터를 남겼다.



# 기록 4 (okt 까지 한번 돌려봄)
=======================================================================================================================
* 사용한 코드
    '''
    데이터 로딩
    '''
    print('Loading data...') # 데이터 로딩
    df_all = csv_reader(file_name) # csv 파일 load
    df_preprocess = df_all.loc[:, ['score','preprocessed_review']] # 점수와 전처리된 리뷰만 가져옴
    df_clean = df_preprocess.dropna(axis=0) # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True) # 인덱스 재정렬
    x1, y1 = df_reindex['preprocessed_review'], df_reindex['score'] # x 리뷰, y 점수

    '''
    okt
    '''
    print('Vectorizing sequence data...') # 데이터 벡터화
    tokenizer = okt_morph(x1) # 전처리된 리뷰 데이터 토크나이징
    return tokenizer


* 결과
    Loading data...
    Vectorizing sequence data...
    100%|██████████████████████████████████████| 3001/3001 [00:14<00:00, 211.03it/s]
    Traceback (most recent call last):
      File "/tmp/pycharm_project_367/DEC-keras/datasets.py", line 389, in <module>
        x, y = load_crawleing_reviews("naver_review_test_data")
    ValueError: too many values to unpack (expected 2)
=======================================================================================================================
* okt_morph() 함수에 시리즈 말고 데이터프레임으로 넣어야 될듯???


# 기록 5
=======================================================================================================================
* 사용한 코드
    # 형태소 분석 함수
    def okt_morph(dataframe):
        # 범위 지정 가능
        df_pre = dataframe['preprocessed_review']
        df_corpus = df_pre[:]

        clean_words = []
        for i in tqdm(df_corpus):
            ok = okt.pos(i)
            words = []
            for word in ok:
                if word[1] not in ['Josa', 'Eomi', 'Punctuation', 'Suffix']:  # 조사, 어미, 구두점이 있는 것은 포함 시키지 않음
                    words.append(word[0])
            clean_words.append(words)

        return clean_words

* 결과
    100%|██████████████████████████████████████| 3001/3001 [00:15<00:00, 196.22it/s]
    리뷰 데이터 : [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '사',
    '통', '팔', '달', '교통', '편리'], ['강력', '추천'], ['매우', '나쁨'], ['구', '성비', '갑', '최고', '호텔'], .. 생략
=======================================================================================================================
* okt 형태소 분류 까지 완료


# 기록 6 (word2vec 예제파일을 따라하며 데이터가 출력되는지 확인해보기)
           * "좋음" 과 비슷한 단어 추출하기
========================================================================================================================
* 사용한 코드
    '''
    word2vec
    '''
    print('Vectorizing sequence data...')  # 데이터 벡터화
    model = Word2Vec(sentences=tokenizer, size=100, window=5, min_count=5, workers=4, sg=0)
    model_result = model.wv.most_similar("좋음")
    print(model_result)


* 결과
    Vectorizing sequence data...
    [('내부', 0.9974992275238037), ('좋습니다', 0.9973698854446411), ('깔끔했고', 0.9973666667938232), ('또한', 0.997318685054779),
    ('친절합니다', 0.9972794651985168), ('넓은', 0.9972636699676514), ('교통', 0.9972415566444397), ('접근성', 0.9972261190414429),
    ('보', 0.9972063899040222), ('방도', 0.9972002506256104)]
    리뷰 데이터 : None
========================================================================================================================
* 데이터가 올바르게 출력되었다. 하지만 word2vec에 대한 이해도가 낮은 상태에서 코드만 넣어서 확인한것이기 때문에
word2vec 공부를 해야함.






# 기록 7 ( 훈련된 모델 shape 확인 )
========================================================================================================================
okt 형태소 분류...
100%|██████████████████████████████████████| 3001/3001 [00:14<00:00, 200.77it/s]
Vectorizing sequence data...
(2065, 100)
========================================================================================================================


# 기록 8 ( 리뷰 통계 : image dir에서 png 확인)
=======================================================================================================================
리뷰의 최대 길이 : 150
리뷰의 평균 길이 : 25.571476174608463
=======================================================================================================================



# 기록 9 ( "만족" 과 비슷한 단어 찾기)
=======================================================================================================================
* sg = 0, min_count = 5
[('많이', 0.9998157024383545), ('정도', 0.9998121857643127), ('사람', 0.9998056888580322), ('식사', 0.9998053312301636),
('된', 0.9998018145561218), ('온도', 0.9998000860214233), ('없고', 0.9997991323471069), ('분위기', 0.9997941851615906),
 ('깨끗한', 0.9997904300689697), ('되는', 0.9997903108596802)]

* sg = 0, min_count = 10
Vectorizing sequence data...
[('도', 0.9998130798339844), ('같은', 0.9998012781143188), ('건', 0.9997950792312622), ('수영장', 0.9997948408126831),
('않아', 0.999783456325531), ('음식', 0.9997806549072266), ('분위기', 0.9997782707214355), ('신경', 0.9997768402099609),
('굉장히', 0.9997766017913818), ('해서', 0.9997742772102356)]

* sg = 0, min_count = 15
[('하지만', 0.999809741973877), ('서', 0.9997804164886475), ('와인', 0.9997660517692566), ('이었습니다', 0.9997588992118835),
('느낌', 0.9997587203979492), ('사람', 0.9997577667236328), ('인테리어', 0.9997565746307373), ('소음', 0.9997557401657104),
('아이', 0.9997552633285522), ('내', 0.9997514486312866)]

* sg = 0, min_count = 20
[('분위기', 0.9997180700302124), ('깨끗한', 0.9997081160545349), ('만족스러웠습니다', 0.9996935129165649), ('깔끔한', 0.9996843338012695),
('와인', 0.9996829628944397), ('큰', 0.9996799230575562), ('면', 0.9996751546859741), ('배려', 0.9996702075004578),
('않지만', 0.9996699094772339), ('않은', 0.9996668100357056)]

* sg = 0, min_count = 25
[('와인', 0.9997572302818298), ('보고', 0.9997364282608032), ('이었습니다', 0.9997293949127197), ('맥주', 0.9997281432151794),
('굉장히', 0.9997214078903198), ('코로나', 0.9997187852859497), ('식사', 0.9997166395187378), ('걸', 0.9997084736824036),
('좋아', 0.9997069239616394), ('문제', 0.9997051954269409)]

* sg = 0, min_count = 30
[('전혀', 0.9997217655181885), ('식사', 0.9997193217277527), ('만족스러웠습니다', 0.9997138977050781), ('였습니다', 0.999710202217102),
('방이', 0.9996978044509888), ('딱', 0.9996961355209351), ('굉장히', 0.9996952414512634), ('않은', 0.9996923208236694),
('볼', 0.9996908903121948), ('제공', 0.9996904730796814)]

************* sg 값 변경 ************************

* sg = 1, min_count = 5
[('친절했고', 0.9958021640777588), ('물론', 0.9955589771270752), ('끔', 0.9953944683074951), ('깔', 0.994544267654419),
('나', 0.9944262504577637), ('친절하시니', 0.9941003918647766), ('친절하세요', 0.9938473105430603), ('만족했습니다', 0.9937354922294617),
('너무나', 0.993686854839325), ('훌륭한', 0.993354856967926)]

* sg = 1, min_count = 10
[('위생', 0.995941162109375), ('깨끗했고', 0.9959219098091125), ('쾌적한', 0.9956857562065125), ('심플', 0.9951194524765015),
('좋았음', 0.9949259757995605), ('훌륭하고', 0.993772029876709), ('고급스러운', 0.993500828742981), ('깨끗해', 0.9931552410125732),
('나', 0.9931240081787109), ('만족스럽습니다', 0.9929053783416748)]

* sg = 1, min_count = 15
[('쾌적한', 0.9949052333831787), ('물론', 0.9939712882041931), ('위생', 0.9927648305892944), ('편한', 0.9920718669891357),
('좋았음', 0.9917362928390503), ('끔', 0.9915804862976074), ('고', 0.9909809827804565), ('친절', 0.9909777641296387),
('고급스러운', 0.9908297061920166), ('정비', 0.9907188415527344)]

* sg = 1, min_count = 20
[('만족스럽습니다', 0.9956712126731873), ('나', 0.9946420192718506), ('쾌적한', 0.9943250417709351), ('편한', 0.9942020773887634),
('물론', 0.9931555390357971), ('보니', 0.9931144118309021), ('고', 0.9930259585380554), ('모던', 0.9926996231079102),
('자체', 0.9923812747001648), ('했고', 0.9920921325683594)]

* sg = 1, min_count = 25
[('보니', 0.9961726665496826), ('고', 0.9959455728530884), ('편', 0.9952077865600586), ('물론', 0.9934786558151245),
('딱', 0.9934673309326172), ('이었는데', 0.9930583834648132), ('않은', 0.9926679134368896), ('도시락', 0.9926109313964844),
('되지', 0.992536723613739), ('되고', 0.9915354251861572)]

* sg = 1, min_count = 30
[('모던', 0.994388222694397), ('대', 0.9939221143722534), ('않은', 0.9937033653259277), ('나', 0.9933357834815979),
('고', 0.9929693937301636), ('그럭저럭', 0.9909741878509521), ('가지', 0.9906245470046997), ('인상', 0.9905768632888794),
('이었는데', 0.9903640151023865), ('이상', 0.9902962446212769)]
========================================================================================================================
* sg = 0 인 경우 min_count 수치를 높을수록 유사성이 높았다.
* sg = 1 인 경우 min_count 수치가 높을수록 유사성이 낮았다.
min_count 수치가 높을수록 학습할수 있는 단어의 개수가 줄어들기 때문에 min_count 수치가 낮을수록 효율이 좋은 sg = 1 값을
사용하는것이 적절하다.





# 기록 10
* okt 품사 태그 확인하여 다양하게 적용해보기
========================================================================================================================
(실행코드)
    print(okt.tagset)

(결과)
    {'Adjective': '형용사', 'Adverb': '부사', 'Alpha': '알파벳', 'Conjunction': '접속사', 'Determiner': '관형사',
    'Eomi': '어미', 'Exclamation': '감탄사', 'Foreign': '외국어, 한자 및 기타기호', 'Hashtag': '트위터 해쉬태그',
    'Josa': '조사', 'KoreanParticle': '(ex: ㅋㅋ)', 'Noun': '명사', 'Number': '숫자', 'PreEomi': '선어말어미',
    'Punctuation': '구두점', 'ScreenName': '트위터 아이디', 'Suffix': '접미사', 'Unknown': '미등록어', 'Verb': '동사'}

* 필요한 품사 : 형용사, 동사, 명사, 부사, 감탄사
                , 관형사(명사의 상황을 나타내줌(새 것, 헌 것)), 미등록어(감정표현이 들어가있는 경우가 있음)
 in {'Adjective', 'Verb', 'Noun', 'Adverb', 'Exclamation', 'Determiner', 'Unknown'}

* 삭제대상 품사 : 접속사, 알파벳, 외국어, 해시태그, 조사(을를이가), ㅋㅋ, 숫자, 구두점,
                  접미사(명사를 꾸며주기 때문에 필요? 떨어져 나온것은 의미가 없다)
                  선어말어미(이미 떨어져나온 선어말 어미라면 의미를 갖기 힘들다), 어미
not in {'Alpha', 'Conjunction', 'Eomi', 'Foreign', 'Hashtag', 'Josa', 'Number', 'KoreanParticle', 'PreEomi', 'Punctuation', 'Suffix'}
========================================================================================================================



# 기록 11
========================================================================================================================
* 기존 품사 분류
    if word[1] not in ['Josa', 'Eomi', 'Punctuation', 'Suffix']:

    okt 품사적용 테스트 :
    [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '사', '통', '팔', '달', '교통', '편리'],
    ['강력', '추천'], ['매우', '나쁨'], ['구', '성비', '갑', '최고', '호텔'], ['좋음'], ['최고'], ['혼캉스', '딱', '좋아요'], ['강력', '추천'],
    ['최고'], ['최고'], ['형편', '없음'], ['가성', '비각', '맞네요'], ['강력', '추천'], ['강력', '추천'], ['매우', '좋음'], ['만족'], ['매우', '좋음'],
    ['만족'], ['강력', '추천'], ['매우', '좋음'], ['서울', '최', '새', '호텔'], ['만족'], ['매우', '좋음'], ['매우', '좋음'], ['매우', '좋음']]


* 수정된 품사 분류
# 감정을 표현하는데 적합한 품사만 가져옴

    if word[1] in ['Adjective', 'Verb', 'Noun', 'Adverb', 'Exclamation', 'Determiner', 'Unknown']:
        # 감정을 표현하는 품사만 따로 가져 오는 것이 유용하다고 판단
          - 어떤 데이터가 있는지 알기 쉽고, 필요에 따라 추가시키기도 편하다.

        # 'Adjective', 'Verb', 'Noun', 'Adverb' : 형용사, 동사, 명사, 부사는 문장 구성 기본이라 필수로 구성.
        # Exclamation : 감탄사는 감정의 가장 단순한 형태여서 꼭 필요하다고 생각
        # Determiner : 관형사는 명사를 꾸며주기 때문에 중요하다고 판단.
        # Unknown : 미등록어는 널리쓰이는 유행어 같은 것을 놓치지 않기 위해 선택
        예시) 어떤 아이가 책을 열심히 읽는다.
        어떤: 관형사, 아이: 명사, 열심히: 부사, 읽는다: 동사
        * skip-gram 형식을 사용하기 때문에 단어의 앞뒤에 밀접하게 붙는 품사가 중요하다.

    okt 품사적용 테스트 :
    [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '통', '달', '교통', '편리'],
    ['강력', '추천'], ['매우', '나쁨'], ['성비', '갑', '최고', '호텔'], ['좋음'], ['최고'], ['혼캉스', '딱', '좋아요'], ['강력', '추천'],
    ['최고'], ['최고'], ['형편', '없음'], ['가성', '비각', '맞네요'], ['강력', '추천'], ['강력', '추천'], ['매우', '좋음'], ['만족'], ['매우', '좋음'],
    ['만족'], ['강력', '추천'], ['매우', '좋음'], ['서울', '최', '새', '호텔'], ['만족'], ['매우', '좋음'], ['매우', '좋음'], ['매우', '좋음']]


    기존 : ['서울', '내', '어디', '사', '통', '팔', '달', '교통', '편리'],
    수정 : ['서울', '내', '어디', '통', '달', '교통', '편리'],
    * 수정된 품사 분류가 필요한 데이터만 가져오는데 더 효과적



* 어근화 적용( 얻는것에 비해 잃는게 많다)
     ok = okt.pos(i, stem=True)
     * 편안하다, 조용하다 같은 동일한 형태로 데이터를 얻지만, word2vec의 경우 군집화를 통해 어근이 같은 문장은 같은 클러스터에
     모이게 되기 때문에 학습데이터가 많아지면 '편해서' 와 '편안하고' 가 굉장히 가까운 위치에 있을 것이기 때문에 어근화를 진행하지 않아도
     된다고 판단했다. 어근화를 진행할경우 학습 데이터의 종류가 적어지기 때문에 미래에 모델이 대응할수 있는 리뷰의 종류도 제한된다고 판단.

    [['형편', '없다'], ['가성', '비각', '맞다'], ['강력', '추천'], ['강력', '추천'], ['매우', '좋다'], ['만족'], ['매우', '좋다'],
    ['만족'], ['강력', '추천'], ['매우', '좋다'], ['서울', '최', '새', '호텔'], ['만족'], ['매우', '좋다'], ['매우', '좋다'], ['매우', '좋다'],
    ['좋다', '다만', '주', '차비', '내다', '거', '이해'], ['좋다', '시간', '보내다', '또', '방문', '싶다'], ['주말', '휴가'], ['강력', '추천'],
     ['강력', '추천'], ['최고'], ['강력', '추천'], ['좋다'], ['실망', '스럽다', '움'], ['매우', '좋다'], ['강력', '추천'], ['매우', '좋다'], ['강력', '추천'],
     ['매우', '좋다'], ['최고'], ['최고'], ['좋다'], ['좋다'], ['조용하다', '편안하다', '휴식'], ['역시', '가든', '호텔'], ['단', '채', '오다', '때', '아침', '어떻다', '합'],
      ['강력', '추천'], ['좋다'], ['최고'], ['만족'], ['강력', '추천'], ['최고'], ['강력', '추천'], ['실망', '스럽다', '움'], ['좋다'], ['최고'],
      ['편리하다', '위치', '누구', '알다', '유명하다'], ['코로나', '이전', '정말', '좋다', '숙소', '이다'], ['매우', '좋다'], ['그럭저럭'], ['최고'],
      ['침대', '밑', '위', '머리카락', '더러', '움'], ['그럭저럭'], ['항상', '편안하다', '숙소'], ['만족'], ['청결하다', '건물', '노후', '다르다'],
      ['매우', '좋다'], ['예전', '조금', '만족', '않다', '하루'], ['오랜', '만', '서울', '오다', '푹', '쉬다'],
      ['기본', '룸', '좀', '더', '비싸다', '가격', '이다', '테라스', '룸', '층', '있다', '고', '층', '원하다', '분', '테라스', '룸비', '추설', '명', '다르다', '안', '돼다', '있다'],
      ['로비', '압도', '당하다', '규모', '역시', '명불허전', '베스트', '웨스턴', '가든', '호텔', '제', '배정', '받다', '곳', '층뷰', '좋다', '모든', '것', '특급', '호텔', '느끼다',
      '충분하다', '곳', '이다', '어느', '것', '하나', '준비', '것', '느껴지다', '새롭다', '생기다', '호텔', '비교', '불가', '이다', '가든', '호텔', '최고', '호텔', '이다'],
      ['주말', '데이트', '찾아가다', '서비스', '좋다', '호텔', '보다', '야경', '좋다', '다', '만', '지하철역', '조금', '떨어지다', '있다', '걸다', '되다'],
      ['작년', '처음', '방문', '후', '휴식', '및', '작업', '차', '다시', '방문', '하다', '재', '방문', '되다', '이유', '위치', '교통', '편', '편리', '버스',




# 기록 12
* word2vec 모델 프린트
============================================================
Vectorizing sequence data...
모델 shape : (2048, 100)
모델 프린트 : Word2Vec(vocab=2048, size=100, alpha=0.025)
============================================================


# 기록 13
* okt 형태소 분석 자료 csv 저장
========================================================================================================================
(실행코드)
def okt_csv(file_name):

    df_all = csv_reader(file_name)  # csv 파일 load
    df_preprocess = df_all.loc[:, ['review_id', 'score', 'review', 'preprocessed_review']]  # 점수, 전처리된 리뷰, 리뷰id(merge할때 필요)
    df_clean = df_preprocess.dropna(axis=0)  # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True)  # 인덱스 재정렬

    x, y = df_reindex['preprocessed_review'], df_reindex['score']  # x 리뷰, y 점수

    print('okt 형태소 분류...')  # 형태소 분류
    df_x1 = pd.DataFrame(x)  # 리뷰 데이터 프레임 변환
    tokenized_riviews = okt_morph(df_x1)  # 전처리된 리뷰 데이터 토크나이징

    dict_okt = {'okt_pos_review': tokenized_riviews}
    df_okt = pd.DataFrame(dict_okt)
    df_merge = pd.merge(df_reindex, df_okt, right_index=True, left_index=True)
    df_merge.to_csv(f"./data/{file_name}_Okt_version.csv", encoding='utf-8')

(csv 형태)
    ,review_id,score,review,preprocessed_review,okt_pos_review
    0,4561805431,4,"""좋음""",좋음,['좋음']
    1,2895377478,4,"""좋음""",좋음,['좋음']
    2,3397527294,5,"""전체적으로 만족도 높음""",전체적으로 만족도 높음,"['전체', '만족도', '높음']"
    3,2687126186,2,"""그럭저럭""",그럭저럭,['그럭저럭']
    4,2779753255,4,"""최고""",최고,['최고']
========================================================================================================================
* 나중에 원본 파일에 join 하기 위해 review_id 컬럼을 함께 가져옴

# 기록 14
==============================
okt 형태소 분류...
100%|██████████| 150552/150552 [05:14<00:00, 478.81it/s]
==================================
* 네이버 리뷰 형태소 분류까지 완료.



# 기록 15
* imdb 데이터셋확인
=================================================
x_train shape: (50000, 1000)
(array([[0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       ...,
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.],
       [0., 1., 1., ..., 0., 0., 0.]]), array([1, 0, 0, ..., 0, 0, 0]))

==================================================


# 기록 16
* 형태소 분석 리뷰 data 벡터화 과정
===================================================
* 리뷰 데이터 형태 2차원 리스트
    ["['좋음']", "['좋음']", "['전체', '만족도', '높음']", "['그럭저럭']", "['최고']", "['서울', '내', '어디', '통', '달', '교통', '편리']", "['강력', '추천']", "['매우', '나쁨']", "['성비', '갑', '최고', '호텔']", "['좋음']", "['최고']"]
* 한개만 출력시
    ['좋음']

* 라벨링 데이터 형태
    [4, 4, 5, 2, 4, 5, 5, 1, 5, 4, 4]
===================================================


# 기록 17
* 모델 세이브
* 전체 리뷰 600만개
===================================================
ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /home/ubuntu/itbiz/user3/DEC/datasets.py
2021-09-27 18:26:43.168251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Loading data...

/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583:
* 경고가 뜬채로 진행이 안되고 있다.
* 파이썬과 넘파이의 충돌이 발생할수 있음을 경고
FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
  종료 코드 -1(으)로 완료된 프로세스
===================================================
* 모델파일은 저장성공하였다.



# 기록 18
* 데이터 벡터화 시도
==================================================================
5 classes
Traceback (most recent call last):
  File "/home/ubuntu/itbiz/user3/DEC/datasets.py", line 502, in <module>
    x, y = load_crawleing_reviews('all_hotels_review_data(final)', model_20)
  File "/home/ubuntu/itbiz/user3/DEC/datasets.py", line 408, in load_crawleing_reviews
    idx_to_key = model.wv.index_to_key
AttributeError: 'Word2VecKeyedVectors' object has no attribute 'index_to_key'

종료 코드 1(으)로 완료된 프로세스
=================================================================
* gpu에서는 index_to_key attribute가 없다.(옛날버전?)



# 기록 19
* 리뷰 데이터 10만개로 word2vec 모델 만들어서 3d 재현 시도(도현쓰)
============================================================================
ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /home/ubuntu/itbiz/user3/DEC/datasets.py
2021-09-28 14:08:05.899418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
100%|█████████████████████████████████| 809737/809737 [04:22<00:00, 3085.37it/s]
==============================================================================
* 모델 save시 입력데이터의 형태에 신경써야 한다
(전체 list 안쪽에 각각의 list, 각각의 list 안에 문자열로 데이터가 들어가있어야함)


# 기록 20
* word2vec 입력 데이터(리스트) 피클파일 형식으로 저장하기
================================================================================
* 사용한 코드
    # 리스트 파일로 저장(피클)
    def save_list(list):
        with gzip.open('review_list.pkl', 'wb') as f:
            pickle.dump(list, f, pickle.HIGHEST_PROTOCOL)

    # 리스트 파일 불러오기(피클)
    def load_list():
        with gzip.open('review_list.pkl', 'rb') as f:
            data = pickle.load(f)
        return data

* 리뷰 10개 테스트 결과
(저장, 로드 모두 정상 작동 확인)
    100%|████████████████████████████████████████| 10/10 [00:00<00:00, 72691.58it/s]
    [['생각', '더러워서', '놀랐습니다'], ['관리', '상태', '양호', '해', '보이진', '않았습니다', '로열', '스위트', '객실'], ['화장실', '문', '안', '닫', '김', '침대', '두', '개', '꾸역꾸역', '들어가', '있어서', '세면대', '세게', '틀', '가까운', '침대', '물이', '튐'], ['지은지', '얼마', '안', '된', '건물', '알', '있는데', '객실', '내', '시설', '상당히', '노후', '화', '관리', '안', '되고', '있는', '상태', '였습니다', '욕조', '침실', '구분', '없어', '엄청', '습하였고', '여기저기', '곰팡이', '보였습니다'], ['변기', '잘', '안', '내려가는', '거', '빼곤', '다', '좋았어요'], ['욕조', '좋아해서', '호텔', '더', '디자이너', '스', '호텔', '체인', '가끔', '이용', '하는데', '이제', '이용', '해본', '더', '디자이너', '스', '호텔', '중', '최악', '이었어요', '서울역', '삼성역', '여기', '좀', '관리', '상태', '별로', '였지만', '군자역', '신', '논현', '점', '다', '이용', '해', '봤는데', '이렇게', '이', '용이', '기분', '나빴을', '정도', '관리', '안', '된', '곳', '홍대', '점', '처음', '요', '관리', '정말', '안', '되어', '있고', '이', '돈', '이', '그냥', '욕조', '있는', '모텔', '낫다', '싶은', '수준', '욕조', '더럽고', '바닥', '부품', '빠져', '있어서', '살', '긁', '힐', '뻔했어요', '수전', '덜컹', '거리', '상태', '최악', '화장실', '문도', '제대로', '안', '닫', '겨', '샤워실', '불', '잘', '안', '들어옴', '리모컨', '하면', '들어옴', '호텔', '자주', '이용', '하는데', '이런', '후기', '남기는', '거', '정말', '처음', '너무', '기분', '나쁜', '주말', '이었어요', '더', '디자이너', '스', '호텔', '다시', '예전', '같은', '수준', '돌아왔으면', '좋겠네요'], ['너무', '추웠는데', '바닥', '미리', '따뜻하게', '데워졌었고요', '욕조', '널찍하면서도', '디자인', '심플', '하게', '예뻤어요', '층', '이었는데', '야경', '꽤', '괜찮았고', '호텔', '바로', '앞', '편의점', '있어서', '편했고요', '조용하고', '이불', '침대', '너무', '포근했네요'], ['호텔', '조용하고', '깔끔했습니다', '직원', '들', '친절하여', '기분', '좋게', '이용', '했습니다', '다음', '또', '방문', '싶습니다'], ['좋긴', '좋은데', '와인', '컵', '제공', '해달라니까', '룸서비스', '이용', '시', '가능하다고', '해서', '종이컵', '먹었어요', '크리스마스', '시즌', '평소', '숙박', '비보', '비싸게', '주고', '잤는데', '흠', '조식', '제공', '되는', '샌드위치', '맛있습니다'], ['무슨', '탕', '비실', '옆방', '준', '건지', '진심', '그렇게', '방음', '안', '되는', '방', '처음', '이었어요', '시끄러워', '요', '새벽', '내내', '잠들', '기', '어려웠어요']]

    종료 코드 0(으)로 완료된 프로세스


* 리뷰 전체 리스트로 저장
100%|███████████████████████████████| 5941314/5941314 [37:20<00:00, 2651.42it/s]

'review_list.pkl'로 저장완료
======================================================================
* 저장한후 load_list() 로 0번째 데이터 형태 확인
* 사용한 코드
    test_list = load_list()
    print(test_list[0])
    print(type(test_list[0]))
* 결과
    ['생각', '더러워서', '놀랐습니다']
    <class 'list'>




# 기록 21
* 클러스터링 결과 출력해봄
===============================================================================
{'리뷰': array([[-4.45704073e-01, -3.66882801e-01,  1.31100342e-01, ...,
        -1.49253443e-01,  2.40333900e-01,  1.52797103e-01],
       [-1.86707556e-01, -2.63934553e-01,  1.05644889e-01, ...,
        -1.12249106e-01,  1.57335445e-01,  1.78158164e-01],
       [-1.15661725e-01,  1.69973262e-03,  8.60931575e-02, ...,
        -4.40693237e-02, -7.90716782e-02, -2.61354586e-03],
       ...,
       [-5.21343667e-04, -1.70271465e-04,  3.80239915e-04, ...,
        -1.88938269e-04,  2.01341463e-05,  1.40015822e-04],
       [-2.01178715e-04, -1.81344498e-04,  1.71244261e-04, ...,
        -1.09002882e-04,  6.75640986e-05,  7.22189434e-05],
       [-1.20891120e-04, -2.69130687e-04,  1.49364176e-04, ...,
         2.45131505e-05,  1.09895715e-04,  8.45251270e-05]]), '라벨링': array([3, 3, 2, 1, 5, 1, 5, 5, 5, 2, 1, 5, 4, 4, 4, 4, 3, 5, 4, 2, 3, 4,
       4, 3, 1, 3, 3, 4, 4, 5, 4, 3, 4, 3, 3, 3, 4, 2, 5, 4, 2, 3, 1, 5,
       1, 4, 2, 2, 4, 4, 1, 5, 3, 3, 5, 4, 5, 3, 5, 3, 5, 4, 1, 1, 4, 2,
       1, 2, 5, 1, 3, 4, 5, 5, 3, 3, 1, 4, 1, 5, 2, 3, 4, 5, 3, 4, 3, 5,
       5, 5, 5, 2, 3, 4, 4, 5, 5, 5, 3, 5, 2, 5, 5, 5, 4, 3, 5, 4, 5, 3,
       1, 1, 1, 5, 5, 2, 1, 5, 4, 4, 2, 2, 4, 4, 5, 5, 5, 2, 4, 4, 2, 5,
       4, 4, 4, 5, 5, 4, 3, 4, 2, 2, 3, 4, 4, 4, 4, 5, 2, 4, 4, 5, 5, 4,
       4, 5, 5, 4, 5, 1, 5, 5, 4, 4, 5, 4, 5, 3, 3, 4, 4, 5, 5, 5, 5, 4,
       4, 3, 5, 5, 3, 4, 5, 4, 4, 2, 4, 3, 4, 5, 5, 5, 4, 4, 4, 4, 2, 4,
       4, 5, 4, 3, 5, 5, 3, 2, 1, 4, 5, 5, 1, 5, 5, 5, 2, 4, 3, 5, 4, 5,
       4, 5, 4, 5, 3, 5, 1, 4, 2, 4, 2, 4, 5, 5, 5, 4, 3, 4, 5, 5, 5, 5,
       3, 4, 5, 5, 3, 5, 3, 4, 2, 5, 3, 5, 4, 5, 4, 3, 5, 5, 5, 3, 5, 4,
       2, 5, 5, 5, 4, 4, 4, 4, 5, 5, 2, 5, 4, 3, 3, 5, 4, 5, 2, 5, 4, 5,
       3, 3, 4, 4, 5, 4, 5, 3, 2, 3, 5, 5, 5, 5, 3, 1, 5, 3, 4, 5, 5, 4,
       3, 3, 4, 4, 5, 5, 2, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5,
       5, 1, 5, 5, 5, 5, 3, 4, 4, 5, 5, 1, 4, 5, 4, 5, 5, 4, 5, 3, 3, 5,
       5, 4, 5, 1, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 3, 5, 4, 5, 3, 5, 5, 5,
       4, 5, 3, 4, 5, 5, 5, 5, 5, 3, 4, 3, 4, 4, 4, 5, 2, 3, 5, 4, 2, 4,
       1, 3, 5, 5, 3, 5, 5, 5, 5, 1, 4, 4, 4, 5, 5, 3, 5, 4, 5, 5, 2, 3,
       4, 3, 1, 3, 4, 5, 5, 5, 5, 4, 4, 5, 5, 3, 5, 3, 1, 5, 5, 3, 3, 3,
       3, 5, 5, 4, 3, 2, 5, 2]), '예측값': array([4, 1, 2, 2, 3, 2, 3, 3, 3, 3, 3, 0, 0, 0, 3, 0, 3, 0, 3, 0, 0, 0,
       0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0])}

종료 코드 0(으)로 완료된 프로세스
=====================================================================================================
* 리뷰가 벡터형태로 나오기 때문에 텍스트로 변환한뒤에 csv 저장을 시도해볼 예정



# 기록 22
* 1000개의 리뷰로 test 진행
  (벡터, 리뷰, 점수, 클러스터넘버)
=====================================================================================================
1. csv 파일로 저장하는데 성공
2. 전체 데이터를 csv로 저장하여 차후에 벡터화하는 시간을 절약할수 있다.
=====================================================================================================



# 기록 23
==================================================================================
* 50만개 test

* min_cnt : 10
* 정확도 : 0.7113321708945689

* min_cnt : 20
* 정확도 : 0.7106447319213277

* min_cnt : 30
* 정확도 : 0.7112969364308394

* min_cnt : 40
* 정확도 : 0.7106962940292918
==================================================================================
* 입력데이터 수정후에 정확도가 올랐다!!


# 기록 24
* 클러스터링 문제 발생??
* 클러스터링이 지나치게 편향적이다.
* 육안으로 확인시 부정적인 리뷰가 긍정 리뷰와 함께 있는것 다수 확인
=====================================================
* min_cnt 10 일떄
    각 스코어 개수
    5    354102
    4     92816
    3     30359
    2     12631
    1      7896
    Name: score, dtype: int64
    각 클러스터 개수
    4    497798
    2         2
    3         2
    0         1
    1         1
    Name: cluster_num, dtype: int64

* min_cnt 30일때
    각 스코어 개수
    5    353521
    4     92694
    3     30292
    2     12615
    1      7880
    Name: score, dtype: int64
    각 클러스터 개수
    0    496979
    3        18
    1         3
    2         1
    4         1
    Name: cluster_num, dtype: int64
=======================================================


# 기록 25
* cluster별로 csv 저장하여 리뷰를 살펴 보았다.
==============================================================
* 사용한 코드

    # 클러스터 num 으로 구분하여 csv 저장후 자료 살펴보기
    df_cluster_all = csv_reader('test_cluster_data_min_cnt_30')
    cluster_num = 5
    for num in range(cluster_num):
        cluster_condition = (df_cluster_all.cluster_num == num)
        df_cluster_each = df_cluster_all[cluster_condition]
        csv_save(df_cluster_each, f'cluster_{num}')

    # 클러스터링 통계 구해 보기
    cluster_num = 5
    for num in range(cluster_num):
        df_test_by_cluster_num = csv_reader(f'cluster_{num}')
        print(f'** cluster_num : {num} **')
        print('** score 분포 **')
        print('==================')
        print(df_test_by_cluster_num['score'].value_counts())
        print('==================')


* 결과

** cluster_num : 0 **
** score 분포 **
==================
5    353509
4     92687
3     30290
2     12614
1      7879
Name: score, dtype: int64
==================


** cluster_num : 1 **
** score 분포 **
==================
4    1
5    1
2    1
Name: score, dtype: int64
==================

** cluster_num : 2 **
** score 분포 **
==================
3    1
Name: score, dtype: int64
==================

** cluster_num : 3 **
** score 분포 **
==================
5    11
4     5
1     1
3     1
Name: score, dtype: int64
==================

** cluster_num : 4 **
** score 분포 **
==================
4    1
Name: score, dtype: int64
==================

종료 코드 0(으)로 완료된 프로세스
=============================================================================================
* cluster 0 번으로 모든 점수의 리뷰가 많이 들어가있다.
* 애초에 5점이 가장 많기 때문에 이런 결과가 생긴 것일까?
* 600만개의 리뷰 데이터에서 score 항목의 분포를 알아보기로 했다.





# 기록 26
* 모든 리뷰 데이터 'total_score' 분포
===============================================================

모든 리뷰 데이터 score 분포(600만개)
5	4321290	0.704 100
4	1179579	0.192 50
3	381660	0.062
2	155983	0.025
1	97404	0.015
sum	6135916
*******************************
테스트 리뷰 데이터 score 분포(50만개)
5	354102	0.711
4	92816	0.186
3	30359	0.06
2	12631	0.025
1	7896	0.015
sum	497804
********************************
테스트 리뷰 클러스터 분포
4	497798	0.999
2	2	    0
3	2	    0
0	1	    0
1	1	    0
sum	497804
=============================================================
* 전체 리뷰의 분포와 random 추출한 50개의 test_data의 비율은 일정하다
* 4점이 약 18~19% 인것에 비해 클러스터링이 너무 부족하게 되었다.
* 실제 리뷰를 읽어보더라도, 5점으로 분류된것 같은 곳에 부정적인 의견의 리뷰가
    다수 발견 되었다.

< 수정된 클러스터링 계획 >
1. 원본 csv 데이터에서 점수별로 나눈다.
2. 각각의 csv 데이터를 이진 분류 클러스터링을 수행, 순수한 데이터를 얻는다.
3. 클러스터링된 csv를 merge하여 다시 하나의 데이터로 만든다.


# 기록 27
* 3점 data 이진 클러스터링
===============================================================

3 classes
Vectorizing sequence data...
벡터화: 100%|█████████████████████████| 363926/363926 [3:39:18<00:00, 27.66it/s]
361134 train sequences
x_train shape: (361134, 100)
y shape: (361134,)
raw_reviews shape: 361134


Pretraining time: 431s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 2)                 4
=================================================================
Total params: 1,307,006
Trainable params: 1,307,006
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 7050
Initializing cluster centers with k-means.

acc: 0.9999916928342388
clustering time:  44.54625368118286


* 클러스터링 결과

    3    361134
    Name: score, dtype: int64
    각 클러스터 개수
    0    361131
    1         3
=================================================================================
* 클러스터링 된 데이터의 개수가 작다.
* 리뷰간 차이점을 알기 어려웠다.
* 분류를 어떻게 해야할지 굉장히 난감한 상황.

* 5개 클러스터는 군집화 결과가 너무 좋지 않았고, 이진분류를 해본뒤 전체적인 판단을 해볼예정
* 5점짜리 리뷰는 너무 많아서 5번을 제외하고 1~4점까지의 리뷰데이터를 이진분류 시도



# 기록 28
* 2점 리뷰
===========================================================================================
2 classes
Vectorizing sequence data...
벡터화: 100%|███████████████████████████| 149639/149639 [40:39<00:00, 61.35it/s]
148875 train sequences
x_train shape: (148875, 100)
y shape: (148875,)
raw_reviews shape: 148875

Pretraining time: 199s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 2)                 4
=================================================================
Total params: 1,307,006
Trainable params: 1,307,006
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 2905
Initializing cluster centers with k-means.

acc: 0.9999865659109992
clustering time:  19.647976875305176


* 클러스터링 결과
    각 스코어 개수
    2    148875
    Name: score, dtype: int64
    각 클러스터 개수
    0    148873
    1         2
Name: cluster_num, dtype: int64

======================================================
* 역시나 별로 .. 효과가 그닥이다.




# 기록 29
    * 극단적인 test data를 만들어서 클러스터링 test 해보기
    * 1점짜리 5점짜리 각 1000개씩 총 2000개의 데이터를 1로 재라벨링해서 테스트
    * 데이터는 최소한 256보다 커야한다. 안그러면 zerodivision에러 난다.
========================================================================
1 classes
Vectorizing sequence data...
벡터화: 100%|████████████████████████████| 2000/2000 [00:00<00:00, 10119.33it/s]
1865 train sequences
x_train shape: (1865, 100)
y shape: (1865,)
raw_reviews shape: 1865

Pretraining time: 6s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 2)                 4
=================================================================
Total params: 1,307,006
Trainable params: 1,307,006
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 35

acc: 0.9935656836461126
clustering time:  2.275355577468872

종료 코드 0(으)로 완료된 프로세스


* 클러스터 결과 확인

*** 1회 시도 ***
각 스코어 개수
1    1865
Name: total_score, dtype: int64
각 클러스터 개수
0    1853
1      12
Name: cluster_num, dtype: int64

*** 2회 시도 ***
acc: 0.9933110367892977
clustering time:  1.9258551597595215
각 스코어 개수
1    897
Name: total_score, dtype: int64
각 클러스터 개수
0    891
1      6
Name: cluster_num, dtype: int64

*** 3회 시도 ***
acc: 0.9954285714285714
clustering time:  1.6110014915466309
각 스코어 개수
1    875
Name: total_score, dtype: int64
각 클러스터 개수
0    871
1      4
Name: cluster_num, dtype: int64

*** 기존 score 점수 바꾸지 않고 진행 ***
acc: 0.5191011235955056
clustering time:  1.6230151653289795
각 스코어 개수
1    467
5    423
Name: total_score, dtype: int64
각 클러스터 개수
0    885
1      5

*** 모델 학습을 10000번 시켜본 결과 ***
acc: 0.5191011235955056
clustering time:  1.8273935317993164
각 스코어 개수
1    467
5    423
Name: total_score, dtype: int64
각 클러스터 개수
0    885
1      5
Name: cluster_num, dtype: int64
* 결과가 소숫점까지 똑같이 나왔다.

=================================================
* 1점과 5점을 반반씩 섞어도 클러스터링이 잘 되지 않았다.




# 기록 30
* 점수별 500개씩 데이터를 추출하여 클러스터링
====================================================
5 classes
Vectorizing sequence data...
벡터화: 100%|█████████████████████████████| 2500/2500 [00:00<00:00, 8187.18it/s]
2297 train sequences
x_train shape: (2297, 100)
y shape: (2297,)
raw_reviews shape: 2297

Pretraining time: 6s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 100)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               50500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 2)                 4002
_________________________________________________________________
clustering (ClusteringLayer) (None, 5)                 10
=================================================================
Total params: 1,307,012
Trainable params: 1,307,012
Non-trainable params: 0


acc: 0.20940356987374836
clustering time:  1.9532287120819092
각 스코어 개수
1    485
2    463
5    458
3    448
4    443
Name: total_score, dtype: int64
각 클러스터 개수
0    2276
3      18
2       1
4       1
1       1
Name: cluster_num, dtype: int64





# 기록 30
* 하이퍼파라미터를 이리저리 바꿔본 결과
===========================================
acc: 0.2350524874486536
clustering time:  2.0478532314300537
각 스코어 개수
1    474
2    447
5    435
4    420
3    415
Name: total_score, dtype: int64
각 클러스터 개수
0    2109
2      68
4      11
3       2
1       1
Name: cluster_num, dtype: int64

* 조건1
* model = Word2Vec(sentences=data, size=300, window=15, min_count=30, workers=4, sg=1, iter=100)  # 모델 학습
acc: 0.23813670004353504
clustering time:  2.3029062747955322
각 스코어 개수
1    485
2    463
5    458
3    448
4    443
Name: total_score, dtype: int64
각 클러스터 개수
0    2184
2      86
4      25
1       1
3       1
Name: cluster_num, dtype: int64

* 조건2
*model = Word2Vec(sentences=data, size=300, window=50, min_count=30, workers=4, sg=1, iter=100)  # 모델 학습
acc: 0.23639529821506314
clustering time:  2.500004529953003
각 스코어 개수
1    485
2    463
5    458
3    448
4    443
Name: total_score, dtype: int64
각 클러스터 개수
0    2196
4      80
3      18
1       2
2       1
Name: cluster_num, dtype: int64
==========================================
* 군집화가 소폭 상승함


# 기록 31
* n_cluster = 3, 마지막 뉴런 개수 = 3
=========================================
acc: 0.2107096212451023
clustering time:  2.280132532119751
각 스코어 개수
1    485
2    463
5    458
3    448
4    443
Name: total_score, dtype: int64
각 클러스터 개수
0    2275
2      21
1       1
Name: cluster_num, dtype: int64

종료 코드 0(으)로 완료된 프로세스
========================================
* 클러스터링 결과가 좋아지지 않음
* 데이터 자체의 문제인가?


# 기록 32
* 조건
    np_size = 300
    model = Word2Vec(sentences=data, size=np_size, window=20, min_count=30, workers=4, sg=1, iter=100)
* 결과
    acc: 0.20161224985821924
    clustering time:  8.500520467758179
    각 스코어 개수
    2    9928
    1    9901
    5    9852
    4    9850
    3    9841
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    49333
    4       26
    2       10
    1        2
    3        1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스
=======================================================================
* 좀 처럼 나아지지 않는 결과. 하이퍼 파라미터의 문제가 아닌것 같다.


# 기록 33
* 어근화 적용해서 형태소 분류한 테스트 파일로 군집화 시도함
===============================================================================
acc: 0.21076233183856502
clustering time:  2.1143314838409424
각 스코어 개수
2    989
1    984
4    983
5    976
3    974
Name: total_score, dtype: int64
각 클러스터 개수
0    4850
4      45
3       6
2       3
1       2
Name: cluster_num, dtype: int64

종료 코드 0(으)로 완료된 프로세스
========================================================
* 어근화를 통해 동일한 단어가 굉장히 많은데도 불구하고 군집화가 잘 이루어지지 않음
* 군집화가 초반에는 이루어지다가, 일정수준이 지나면 분류를 하지 않고 나머지 데이터가
특정 클러스터로 몰리는것 같음!!!




# 기록 34
* 5개로 클러스터를 진행했을때 대부분이 몰려있고, 아웃라이어같은 소수 데이터가
다른 군집으로 여겨지는것 같음.
* 클러스터의 개수를 늘려서 다시 해봐야겠다.
==========================================================================
* 조건(클러스터 개수 15개)
    acc: 0.26763147166734613
    clustering time:  10.086501121520996
    각 스코어 개수
    2    989
    1    984
    4    983
    5    976
    3    974
    Name: total_score, dtype: int64
    각 클러스터 개수
    13    4419
    12     378
    5       50
    14      19
    6       17
    11       8
    3        7
    4        2
    8        1
    1        1
    9        1
    2        1
    10       1
    7        1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스
===============================================================
* 26프로! 클러스터 개수 여러개로 했더니 꽤 오름!!!
* 아웃라이어 또는 개수가 30개 미만인 데이터는 군집화하지 않는 방향으로 하면 어떨까?
* 일단 클러스터 개수를 늘려서 어떤 결과가 나오는지 몇번 더 테스트 해볼예정


# 기록 35
==============================
* 조건(클러스터 개수 25개)

acc: 0.2317570322054627
clustering time:  29.6826651096344
각 스코어 개수
2    989
1    984
4    983
5    976
3    974
Name: total_score, dtype: int64
각 클러스터 개수
14    4607
10     161
22      50
17      24
6       13
24      11
13       7
15       6
23       6
3        5
21       3
19       3
9        2
7        2
4        2
12       1
5        1
11       1
1        1
===================================================================
클러스터 개수 15개일때 보다 결과가 좋지 않음
* 20개로 재시도




# 기록 36(20211009)
* 단어의 벡터화부터 다시 해보기
============================================================
* 사용한 코드
    file_name = 'test_csv_50'
    data = reviews_parcing(file_name)
    model = Word2Vec(data[:10], min_count=1)  # 모델 학습iter=100
    words = list(model.wv.vocab)
    print(words)
    print(model['항상'])

* 결과
    ['성비', '좋은', '괜찮습니다', '항상', '좋습니다', '생각', '넓고', '괜찮았어요', '화장실', '깨끗한', '편이', '고요', '다만', '이불', '제대로', '세탁', '하지', '않았는지', '얼룩', '많은', '편이었어요', '무엇', '열차', '많이', '지나가다', '보니', '너무', '소음', '심해서', '중간', '깨서', '깊이', '잠들', '기는', '힘들었어요', '아침', '토스트', '좋아요', '잘', '쉬', '갑니다', '젛습니', '리', '모델링', '호텔', '침구', '류', '좋았어요', '조식', '간단했지만', '댤걀', '프라이', '바로', '해주시', '친절하셔서', '만족', '롯데', '시티', '구로', '점', '첫', '방문', '했는데', '만족스러워서', '리뷰', '쓰게', '되었어요', '우선', '깔끔한', '외관', '층', '프런트', '객실', '시설', '만족스러웠습니다', '체크', '해주신', '분도', '친절하셨어요', '마스크', '끼', '일', '하시는데도', '뚫고', '나오는', '친절함이었어요', '후', '향', '하는데', '뵈었던', '다른', '직원', '다', '들', '인사', '해주시고', '친절하셨습니다', '친절하시고', '깔끔하고', '필요한', '것', '구비', '되어', '있어서', '정말', '비즈니스', '목적', '물론', '친구', '놀러', '오시', '기도', '좋을', '같아요', '깔끔하', '곺']
    [-2.8765241e-03 -3.6248378e-03  2.9458459e-03 -1.3829821e-03
      3.8572985e-03 -3.6576048e-03 -3.6990736e-03 -1.4548040e-03
     -4.0970813e-03 -1.5524781e-03 -4.6029887e-03 -4.2221523e-03
      3.4751701e-03  2.8685932e-03  4.7984887e-03  3.9883007e-04
      1.4688533e-03 -1.4477281e-03  2.3197858e-03  7.1745925e-04
      1.4423773e-03  4.5301979e-03 -3.6202224e-03  3.8448137e-03
     -1.2511349e-03 -3.9921859e-03 -5.8000354e-04  2.6635414e-03
     -5.7952892e-04  4.3256460e-03 -4.1082193e-04 -2.4594360e-03
     -2.3908555e-03 -4.1900356e-03  4.6459860e-03 -1.7153631e-03
      1.5764034e-03 -4.9701268e-03 -2.3093822e-03 -1.1256038e-03
      1.0846133e-03  3.4957898e-03 -1.1952586e-03  4.5584608e-03
     -9.1761985e-04  2.9268381e-03 -1.0878923e-03  4.6873270e-03
     -2.6050173e-03  3.7861106e-03 -4.6743280e-03 -4.6999222e-03
     -3.7274717e-03 -1.1110714e-03 -2.7270713e-03 -4.8162872e-03
      1.4081654e-03 -4.5644064e-03 -1.0818267e-03 -3.8778924e-04
     -4.8068142e-03 -4.7855619e-03  1.7579847e-03 -3.4741338e-03
      3.1827716e-03 -1.2302276e-03 -9.8318415e-06 -4.9175485e-03
      2.0907228e-03 -1.1360480e-03 -1.7785085e-03 -3.1590490e-03
     -3.5810429e-03  1.5330387e-03  5.1341974e-04  2.2140769e-03
      3.4527669e-03  2.9375784e-03 -2.9622954e-03 -2.7660683e-03
      1.2622316e-03 -2.0363731e-03 -1.5426575e-03  2.7297745e-03
     -1.0816103e-03 -2.5850108e-03  4.9769706e-03 -2.7941826e-03
      1.7821593e-03 -3.5925175e-03 -3.3106832e-03  4.0577836e-03
      2.4083934e-03  4.8534758e-03 -3.9608171e-04 -3.0922939e-03
     -4.4220407e-03 -4.4006039e-03 -1.1508226e-03  1.5253545e-03]





# 기록 37
======================================================================
* 모델 학습후 유사도 파악하기(랜덤데이터 5000개)

* 사용한 코드
def load_crawling_data():
    file_name = 'test_review_data_5000'
    data = reviews_parcing(file_name)
    model = Word2Vec(sentences=data[:], size=100, window=15, min_count=5, workers=4, sg=1)  # 모델 학습iter=100

    words = list(model.wv.vocab)
    idx_to_key = model.wv.index2word

    model_result_good = model.wv.most_similar("좋아요")  # 어떤 단어와 비슷한지 확인
    model_result_bad = model.wv.most_similar("불친절")  # 어떤 단어와 비슷한지 확인

    print(model_result_good)
    print(model_result_bad)

* 결과
- 좋아요
[('깨끗하고', 0.9151336550712585),('따뜻하고', 0.9134659767150879), ('넓고', 0.879665732383728),
('깔끔하고', 0.8765496015548706), ('깔끔해서', 0.8746862411499023), ('좋아', 0.8667066097259521),
('좋습니다', 0.8578236103057861), ('깨끗해서', 0.849573016166687), ('넓어서', 0.8492798805236816),
('좋네요', 0.8416770696640015)]

- 불친절
[('임', 0.9735562205314636), ('별로', 0.9689991474151611), ('였어요', 0.9263584017753601),
('엉망', 0.8531590104103088), ('이었어요', 0.8170286417007446), ('였음', 0.8052639961242676),
('였습니다', 0.8043773174285889), ('최악', 0.8042610287666321), ('수준', 0.7874618768692017),
('여관', 0.7874157428741455)]
======================================================================
* 유사한 단어와 많이 매칭 되는것을 확인


# 기록 38
======================================================================
* 1점, 5점 순도높은 데이터 추출(총 200개)

* 사용한 코드
def load_crawling_data():
    file_name = 'cleandata_labeled_1_5'
    data = reviews_parcing(file_name)
    model = Word2Vec(sentences=data[:], size=100, window=15, min_count=5, workers=4, sg=1)  # 모델 학습iter=100

    words = list(model.wv.vocab)
    idx_to_key = model.wv.index2word

    model_result_good = model.wv.most_similar("최고")  # 어떤 단어와 비슷한지 확인
    model_result_bad = model.wv.most_similar("별로")  # 어떤 단어와 비슷한지 확인

    print(model_result_good)
    print(model_result_bad)

* 결과
- 최고
[('번', 0.9993466734886169), ('느낌', 0.9993159174919128), ('사람', 0.9993124604225159),
('적', 0.9993082284927368), ('정말', 0.9993053674697876), ('서비스', 0.999305009841919),
('다', 0.9993036985397339), ('바닥', 0.9993003010749817), ('안', 0.9992914199829102),
('움', 0.999286949634552)]

- 별로
[('물', 0.9995040893554688), ('아주', 0.9994841814041138), ('확인', 0.999474048614502),
('시간', 0.9994694590568542), ('정말', 0.9994577169418335), ('한', 0.9994505643844604),
('사람', 0.9994488954544067), ('해', 0.9994344115257263), ('안', 0.999428391456604),
('곳', 0.9994221925735474)]

===================================================
* 유사한 단어와 매칭되지 않는것을 확인(데이터의 개수가 적어서 그런것으로 추정)


# 기록 39
* pre_train
* 순도높은 데이터 300개 클러스터링 결과
====================================================
acc: 0.5034246575342466
clustering time:  1.1460070610046387
각 스코어 개수
5    148
1    144
Name: total_score, dtype: int64
각 클러스터 개수
0    289
1      3
Name: cluster_num, dtype: int64
=====================================================
* 결과에 변화가 없다.




# 기록 39-1
* pretrain 배치 사이즈 조정 256 > 32
==========================================================================
Update interval 30
Save interval 45
Initializing cluster centers with k-means.
Iter 0: acc = 0.53767, nmi = 0.07145, ari = 0.00494  ; loss= 0
saving model to: results/DEC_model_0.h5
Iter 30: acc = 0.53767, nmi = 0.07145, ari = 0.00494  ; loss= 4e-05
delta_label  0.0 < tol  0.001
Reached tolerance threshold. Stopping training.
saving model to: results/DEC_model_final.h5
acc: 0.5376712328767124
clustering time:  1.1650488376617432
csv 파일화...: 100%|██████████████████████| 292/292 [00:00<00:00, 827524.84it/s]
각 스코어 개수
5    148
1    144
Name: total_score, dtype: int64
각 클러스터 개수
0    279
1     13
Name: cluster_num, dtype: int64

종료 코드 0(으)로 완료된 프로세스
============================================================================
* 크게 성능이 좋아지지 않음




# 기록 40
* ae_weights 파일 삭제후
* pre_train epoch 수치를 default=50으로 설정하여 순도높은 데이터 클러스터링 >> ae_weights 파일 생성
* 5000개의 테스트 데이터 클러스터링을 시도
========================================================
Pretraining time: 10s
Pretrained weights are saved to results/ae_weights.h5
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 200)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               100500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 5)                 10005
_________________________________________________________________
clustering (ClusteringLayer) (None, 5)                 25
=================================================================
Total params: 1,363,030
Trainable params: 1,363,030
Non-trainable params: 0
_________________________________________________________________
Update interval 30
Save interval 90
Initializing cluster centers with k-means.
Iter 0: acc = 0.20378, nmi = 0.00250, ari = -0.00006  ; loss= 0
saving model to: results/DEC_model_0.h5
Iter 30: acc = 0.20357, nmi = 0.00275, ari = -0.00007  ; loss= 0.0
delta_label  0.0006295907660020986 < tol  0.001
Reached tolerance threshold. Stopping training.
saving model to: results/DEC_model_final.h5
acc: 0.20356768100734524
clustering time:  1.6571869850158691
csv 파일화...: 100%|████████████████████| 4765/4765 [00:00<00:00, 829529.68it/s]
각 스코어 개수
1    972
2    968
4    942
5    942
3    941
Name: total_score, dtype: int64
각 클러스터 개수
0    4697
4      51
2      15
1       1
3       1
Name: cluster_num, dtype: int64

종료 코드 0(으)로 완료된 프로세스

===================================================================
* pretrain 가중치를 load하지 못하는것 같음
* 매번 코드 실행시 pretrain을 가중치를 새롭게 저장??





# 기록 41
* 새로운 프로세스
1. 모든 가중치 삭제 >> 순도 높은 데이터로 pretrain >> 초기 가중치 얻기 >> 초기 가중치 테스트 데이터로 업데이트
2. x = np.empty((np_size,), dtype='float64') >> dtype float32 에서 64로 변경(좀더 정확하게 벡터표현이 가능)
===============================================================
* 1회차(가중치 없음_pretrain으로 새롭게 만들기)
    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    289
    1      3
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스
    >> 역시나 좋지 않은 결과

* 2회차(1만개 테스트 리뷰 데이터)
    각 스코어 개수
    1    1978
    2    1975
    5    1958
    3    1946
    4    1945
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    9785
    3      11
    2       4
    1       1
    4       1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스
    >> 가중치를 업데이트하면서 했음에도 결과가 나아지지 않음

================================================================






# 기록 42
========================================================
* 인터프리터 오류(can't get remote)
* 원격호스트에서 파일을 복사하고 붙혀넣을 경우 발생
>> 인터프리터 모두 지우고 새로 설정하여 해결함
==========================================================







# 기록 43
* 순도높은 데이터로 테스트 진행
=====================================================================================
* 반복해서 돌릴경우 성능이 좋아진다는 내용을 토대로 테스트 데이터의 iter 수치를 매우 높게 주고 테스트 진행
def load_crawling_data():
    np_size = 200
    window_size = 30
    min_count_size = 5
    sg_type = 0 # 1 = skip-gram, 0 = CBOW
    hs_type = 0 # default값 0, 1이면 softmax 함수 사용
    iter_cnt = 100 # 모델 학습 횟수

    file_name = 'cleandata_labeled_1_5'

* 에러 발생
    ValueError: Cannot assign to variable encoder_0/kernel:0 due to variable shape (200, 500) and value shape (300, 500) are incompatible
* pretrain으로 가중치 만들때 np_size 300으로 설정하고 다시 차원수를 200으로 바꾸면 벡터 차원수가 달라서 오류가 난다.
* 차원은 적절하다고 생각되는 값으로 설정하면 되도록 바꾸지 않는것이 좋겠다.
====================================================================================


# 기록 44
* 초기 가중치 업데이트(300개의 리뷰데이터)
====================================================================================
* 순도높은 데이터를 통한 초기 가중치 업데이트
def load_crawling_data():
    np_size = 200
    window_size = 30
    min_count_size = 5
    sg_type = 1 # 1 = skip-gram, 0 = CBOW
    hs_type = 1 # default값 0, 1이면 softmax 함수 사용
    iter_cnt = 5000 # 모델 학습 횟수

* 결과
    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    288
    1      4
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스
====================================================================
* 결과가 좋지 못하다


# 기록 45
==========================================================================
* 논문 자료의 인용구
(다차원 데이터에 대한 심층 군집 네트워크의 성능향상 방법, 이현진, Journal of Korea Multimedia Society Vol. 21, No. 8, August 2018(pp. 952-959))
1. 군집화 알고리즘은 입력 데이터의 형태에 따라서 성능차이가 크게 발생한다.
2. 데이터의 차원이 클때는 군집화 알고리즘의 변경이나 유사도 평가 방법의 변경으로는
   우수한 성능을 기대할 수 없다.

* 데이터 차원을 100으로 축소
    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    286
    1      6
    Name: cluster_num, dtype: int64

* 데이터 차원을 50으로 축소
    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    291
    1      1
    Name: cluster_num, dtype: int64


* 데이터 차원을 25로 축소
    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    272
    1     20
    Name: cluster_num, dtype: int64
    * 클러스터링 성능이 소폭 상승

* 데이터 차원을 10으로 축소
    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    291
    1      1
    * 10은 너무 작았던것 같다

=======================================================================================
* 100이하로 했을경우에 더 좋다고 판단하기 어려움
* 100 ~ 200의 차원이 적절하다고 판단



# 기록 46
=========================================================================================
* epoch과 updtae_interval이 None이었다. 데이터가 학습을 거의 하지 못한 상태라고 판단했다.
(pretrain은 최초 가중치를 얻는 과정이기 때문이 None으로 설정)

    Namespace(ae_weights=None, batch_size=32, dataset='crawling_reviews', maxiter=20000.0,
    pretrain_epochs=None, save_dir='results', tol=0.001, update_interval=None)

* batch_size는 64로 설정 (32, 64가 성능이 가장 좋고 크기가 크면 학습이 안정적이지만,
                            일반화성능이 감소하는 경우가 있다고함)
                            * 일반화 성능 : 새로운 데이터에서도 잘 작동하는가?

* 재설정한 내용
    parser.add_argument('--batch_size', default=64, type=int)
    parser.add_argument('--maxiter', default=2e4, type=int)
    parser.add_argument('--pretrain_epochs', default=50, type=int)
    parser.add_argument('--update_interval', default=30, type=int)

* 결과
    Namespace(ae_weights=None, batch_size=64, dataset='crawling_reviews', maxiter=20000.0,
                pretrain_epochs=50, save_dir='results', tol=0.001, update_interval=30)

    각 스코어 개수
    5    148
    1    144
    Name: total_score, dtype: int64
    각 클러스터 개수
    1    289
    0      3
    Name: cluster_num, dtype: int64
================================================================
* 결과가 나아지지 않았다



* 기록 47
한번 더 pre_train을 통해 얻은 가중치를 이용하는지 여부에 따른 클러스터링 성능 비교
=============================================================================
* 순도높은 데이터로 2진분류를 해서 얻은 가중치를 사용했을 경우
(1점 2500개, 5점 2500개의 테스트 데이터 사용)
    acc: 0.5064962279966471
    clustering time:  2.0631442070007324
    csv 파일화...: 100%|████████████████████| 4772/4772 [00:00<00:00, 703646.29it/s]
    각 스코어 개수
    1    2419
    5    2353
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    4770
    1       2
    Name: cluster_num, dtype: int64
==============================================================================
* 결과가 좋지 않다.

구글링을 통해 알아낸 클러스터링이 잘 되지 않는 이유로는
1. 데이터 자체의 특성이 없어서(약해서)
2. outlier에 취약한 k-means의 특성
위 두가지 상황이라고 생각.



* 기록 48
============================================================================
* 아웃라이어에 취약한 k-means의 특성을 고려하여 새로운 방법을 시도
    - 클러스터 개수를 50개로 늘려보았다.
    - 개수가 적은 것은 아웃라이어 처리를 하고 상위 그룹의 실제 데이터를 살펴보고 군집화 상황을 판단

* 결과
    acc: 0.6861610968294773
    clustering time:  30.835030794143677
    csv 파일화...: 100%|████████████████████| 4668/4668 [00:00<00:00, 542039.56it/s]
    각 스코어 개수
    1    2395
    5    2273
    Name: total_score, dtype: int64
    각 클러스터 개수
    16    3015
    32    1153
    24     218
    5       74
    37      66
    35      27
    45      22
    18      14
    20      11
    49       9
    8        9
    43       9
    27       8
    31       7
    17       4
    9        3
    14       3
    12       3
    39       3
    13       2
    19       2
    23       2
    1        1
    30       1
    3        1
    4        1
    Name: cluster_num, dtype: int64

* 실제 데이터를 살펴보기 위해 상위 클러스터 csv 저장
* 사용한 코드

    # 클러스터 num 을 선택적하여 저장하기
    df_cluster_all = csv_reader('cluster_50_review_data')
    cluster_numbers = [16, 32, 24]
    for cluster_number in cluster_numbers:
        cluster_condition = (df_cluster_all.cluster_num == cluster_number)
        df_cluster_each = df_cluster_all[cluster_condition]
        csv_save(df_cluster_each, f'cluster_50_review_data_cluster_num_{cluster_number}')


* 16번 클러스터 데이터 분포
=============================
5    2161
1     854

* 32번 클러스터 데이터 분포
=============================
1    1042
5     111

* 24번 클러스터 데이터 분포
=============================
1    217
5      1
=====================================================================================
* 16번 클러스터의 5점 리뷰 2161 개는 데이터 특성이 유사한 5점 리뷰라고 볼 수 있다.
    - 16번 클러스터의 1점 리뷰 854개는 데이터 특성이 유사하지만 5점과 함께 분류 되었다.(아웃라이어 처리 가능)

* 32번 클러스터의 1점 리뷰 1042 개는 데이터 특성이 유사한 1점 리뷰라고 볼 수 있다.
    - 32번 클러스터의 5점 리뷰 111개는 1점과 유사하게 보이는 5점이다(특성이 약함)

* 사람의 표현력이 다양하기 때문에 유사성이 낮은 리뷰는 제대로 군집화 되지 않은 것을 알수 있다.




(20211015)
# 기록 49
* 1점 리뷰 30000개(테스트 데이터)
* cluster 10, 15, 30개로 테스트후에 데이터 살펴보고
* cluster 개수를 구하기
* DEC 시행 횟수에 따라 cluster 개수를 변하게 하는 것도 괜찮은것 같음
- 처음에 아웃라이어를 과감하게 제거하고 점차 신중하게 제거하는 방식
cluster_cnt 변화 예시 : 50 > 40 > 30 > 20 > 10
cluster_cnt 변화 예시 : 30 > 25 > 20 > 15 > 10
- 상위 분류되지 않은데이터는 따로 모아두기
- 나중에 모아둔 데이터만 분류하여 의미있는 데이터가 있는지 볼수 있음
=====================================================
* 클러스터링 1회차
    - n_cluster = 50
    - 상위 클러스터 csv 저장후 데이터 살펴보기

    acc: 0.7073681366921051
    clustering time:  756.936402797699
    csv 파일화...: 100%|██████████████████| 29614/29614 [00:00<00:00, 405805.35it/s]
    각 스코어 개수
    1    29614
    Name: total_score, dtype: int64
    각 클러스터 개수
        10    20948
        39     7187
        24      773
        37      243
        14      132
        32       91
        49       36
        7        34
        36       28
        44       27
        46       12
        43       12
        16       10
        4         8
        20        6
        28        6
        48        6
        31        6
        11        5
        2         5
        41        5
        34        4
        22        4
        17        4
        35        4
        25        3
        12        3
        40        2
        5         2
        8         1
        18        1
        23        1
        3         1
        45        1
        1         1
        6         1
        15        1
        Name: cluster_num, dtype: int64

        종료 코드 0(으)로 완료된 프로세스



    * 클러스터링 2회차
        - n_cluster = 40
        - 상위 클러스터 csv 저장후 데이터 살펴보기

    acc: 0.9369957741067998
    clustering time:  567.8060052394867
    csv 파일화...: 100%|██████████████████| 20824/20824 [00:00<00:00, 316039.25it/s]
    각 스코어 개수
    1    20824
    Name: total_score, dtype: int64
    각 클러스터 개수
    0     19512
    15      325
    16      270
    11      144
    36      124
    22       87
    30       70
    38       49
    29       38
    35       31
    7        26
    33       21
    32       17
    24       17
    10       13
    31       11
    5         9
    34        8
    20        8
    37        8
    14        7
    28        4
    21        4
    25        3
    2         3
    17        3
    4         2
    19        2
    9         2
    3         1
    6         1
    26        1
    18        1
    12        1
    1         1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스



* 클러스터링 3회차
    - n_cluster = 30
    - 상위 클러스터 csv 저장후 데이터 살펴보기
    ('좋아요'가 아직 조금 있는데'안','좋아요' 때문으로 추측)


    acc: 0.9906585228147616
    clustering time:  13.577826738357544
    csv 파일화...: 100%|██████████████████| 19483/19483 [00:00<00:00, 546858.58it/s]
    각 스코어 개수
    1    19483
    Name: total_score, dtype: int64
    각 클러스터 개수
    11    19301
    23       78
    18       41
    8        19
    25        7
    27        5
    7         5
    19        4
    26        3
    17        2
    28        2
    9         1
    14        1
    13        1
    12        1
    10        1
    16        1
    24        1
    1         1
    22        1
    6         1
    21        1
    5         1
    4         1
    3         1
    2         1
    15        1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스



* 클러스터링 4회차
    - n_cluster = 20
    - 상위 클러스터 csv 저장후 데이터 살펴보기
    (7번 클러스터에 아직도 '좋아요'가 존재함)


    acc: 0.967965996267883
    clustering time:  727.8978569507599
    csv 파일화...: 100%|██████████████████| 19292/19292 [00:00<00:00, 364096.82it/s]
    각 스코어 개수
    1    19292
    Name: total_score, dtype: int64
    각 클러스터 개수
    7     18674
    18      370
    14      120
    5        53
    16       25
    8        13
    15       11
    2         7
    4         4
    10        4
    12        4
    17        3
    1         1
    3         1
    6         1
    9         1
    Name: cluster_num, dtype: int64

종료 코드 0(으)로 완료된 프로세스




* 클러스터링 5회차
    - n_cluster = 10
    - 상위 클러스터 csv 저장후 데이터 살펴보기
    ('좋아요' 데이터 일부가 사라지지 않음)
    ('안','좋아요' 외에도 '위치는','좋아요','다른건','다','최악' 같은 리뷰 때문에 같은 군집으로 되는것 같음)

    acc: 0.9879505167889466
    clustering time:  11.819744348526001
    csv 파일화...: 100%|██████████████████| 18673/18673 [00:00<00:00, 663387.90it/s]
    각 스코어 개수
    1    18673
    Name: total_score, dtype: int64
    각 클러스터 개수
    0    18448
    8      133
    6       56
    9       15
    4       10
    7        7
    1        1
    2        1
    3        1
    5        1
    Name: cluster_num, dtype: int64


* 클러스터링 테스트 마지막
    - n_cluster = 30
    - 아웃라이어 처리되었던 리뷰들을 모두 모아서 클러스터링 시도후 결과를 살펴보기


acc: 0.9899026653324843
clustering time:  3.9830846786499023
csv 파일화...: 100%|██████████████████| 10993/10993 [00:00<00:00, 552701.10it/s]
각 스코어 개수
1    10993
Name: total_score, dtype: int64
각 클러스터 개수
0     10882
14       37
15       20
26        9
22        7
13        4
5         4
18        3
28        3
24        2
17        2
25        2
8         2
7         1
20        1
6         1
21        1
19        1
12        1
4         1
27        1
11        1
3         1
10        1
2         1
9         1
1         1
16        1
23        1
Name: cluster_num, dtype: int64

종료 코드 0(으)로 완료된 프로세스

=================================================================================================
* 1점 리뷰 30000개로 테스트 진행
1. n_cluster = 50 > 40 > 30 > 20 > 10
2. 가장 개수가 많은 군집만 택하여 다음단계 클러스터링 진행
3. 탈락한 군집은 따로 모아서 저장
4. 클러스터링 데이터 = 약 18000개 (약 60%)
   아웃라이어 데이터 = 약 11000개 (약 40%)

5. 최종 데이터에 검색기능으로 '좋아요' 댓글이 약 30~40개 정도 존재
- '안좋다'와 '좋지만 다 최악' 이라는 댓글이 많기 때문에 벡터화 시켰을때 가까운 위치에 있는 것으로 추청
- 이것을 효과적으로 제거 시킬수 있는 방식에 대해 연구해봐야함

6. 아웃라이어 데이터 클러스터링 시도(n_cluster = 30)
- 0번 클러스터에 98%의 데이터(약 1만개)가 모여있음
- 2000 인덱스 부터는 길이가 굉장히 긴 리뷰가 대부분


7. 동일한 데이터인데 군집화가 함께 이루어지지 않은 점이 의문

* 최종 클러스터링 데이터
7505,"""매우 나쁨""","['매우', '나쁨']",1,0

* 아웃라이어 데이터
7376,"""매우 나쁨""","['매우', '나쁨']",1,0

8. 최종 클러스터링 데이터에 포함되어 있는 소량의 오류 데이터 제거와
   아웃라이어에 포함 되어있는 해당 점수에 적합한 리뷰 추출이 해결 과제로 남아 있음

9. 향후 클러스터의 개수를 재조정해서 테스트를 진행해볼 예정



(20211018)
# 기록 50
- 팀원과 정확도 차이가 9% 발생
- 하이퍼 파라미터가 조금 달랐음
- 하이퍼 파라미터 동일하게 세팅하고 테스트 진행
=====================================================================
* 1회차 결과
    acc: 0.9632606199770379
    clustering time:  16.575406074523926
    csv 파일화...: 100%|██████████████████| 29614/29614 [00:00<00:00, 526129.03it/s]
    각 스코어 개수
    1    29614
    Name: total_score, dtype: int64
    각 클러스터 개수
    11    28526
    21      576
    6       137
    35      105
    41       59
    14       43
    30       25
    43       22
    16       14
    9        12
    23       11
    46        8
    17        8
    32        8
    47        7
    31        6
    36        6
    37        4
    38        4
    22        3
    13        3
    4         3
    10        2
    33        2
    26        2
    39        2
    19        1
    40        1
    5         1
    3         1
    27        1
    2         1
    12        1
    28        1
    44        1
    8         1
    49        1
    20        1
    1         1
    15        1
    7         1
    42        1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스


* 2회차 결과
    acc: 0.6797798338623624
    clustering time:  746.7735388278961
    csv 파일화...: 100%|██████████████████| 29614/29614 [00:00<00:00, 325964.61it/s]
    각 스코어 개수
    1    29614
    Name: total_score, dtype: int64
    각 클러스터 개수
    34    20131
    19     7825
    49      728
    30      326
    6       208
    40      109
    25       73
    13       35
    44       29
    32       22
    21       18
    20       14
    42       12
    43        9
    3         7
    45        6
    35        6
    14        5
    46        5
    27        5
    15        5
    22        4
    29        3
    47        3
    8         3
    23        3
    36        3
    38        2
    2         2
    17        2
    31        2
    39        2
    26        1
    10        1
    11        1
    1         1
    7         1
    4         1
    18        1
    Name: cluster_num, dtype: int64

    종료 코드 0(으)로 완료된 프로세스

* 3회차 결과
    acc: 0.8569257783480786
    clustering time:  761.4282655715942
    csv 파일화...: 100%|██████████████████| 29614/29614 [00:00<00:00, 311947.78it/s]
    각 스코어 개수
    1    29614
    Name: total_score, dtype: int64
    각 클러스터 개수
    29    25377
    38     2937
    11      617
    27      205
    44      117
    34       75
    7        60
    43       35
    40       27
    23       22
    49       22
    32       15
    5        12
    39       12
    13       11
    6         8
    18        7
    12        6
    26        6
    48        6
    35        5
    20        5
    36        3
    3         2
    37        2
    24        2
    9         2
    25        2
    41        2
    28        2
    22        2
    14        1
    16        1
    42        1
    8         1
    4         1
    2         1
    1         1
    31        1
    Name: cluster_num, dtype: int64


=============================================
* 같은 데이터로 돌릴때마다 결과가 다름
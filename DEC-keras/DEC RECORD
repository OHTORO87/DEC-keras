# 기록 1
* GPU 할당 번호를 바꿈
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
* mnist 테스트
========================================================================================================================
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 784)]             0
_________________________________________________________________
encoder_0 (Dense)            (None, 500)               392500
_________________________________________________________________
encoder_1 (Dense)            (None, 500)               250500
_________________________________________________________________
encoder_2 (Dense)            (None, 2000)              1002000
_________________________________________________________________
encoder_3 (Dense)            (None, 10)                20010
_________________________________________________________________
clustering (ClusteringLayer) (None, 10)                100
=================================================================
Total params: 1,665,110
Trainable params: 1,665,110
Non-trainable params: 0
_________________________________________________________________
Update interval 140
Save interval 1365
Initializing cluster centers with k-means.


Iter 5880: acc = 0.96059, nmi = 0.90227, ari = 0.91529  ; loss= 0.11461
delta_label  0.0009 < tol  0.001
Reached tolerance threshold. Stopping training.
saving model to: results/DEC_model_final.h5
/home/ubuntu/anaconda3/envs/tf2_py38_DEC/lib/python3.8/site-packages/sklearn/utils/linear_assignment_.py:124: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.
  warnings.warn(
acc: 0.9605857142857143
clustering time:  180.99016213417053
========================================================================================================================



# 기록 2 ( score 편집 )
========================================================================================================================
* 네이버 score 편집
    2로 나누고 > round > 0점인 경우 1로 바꿈 >int
* 사용한 코드

    # 데이터 부르기
    df_naver = csv_reader("naver_reiview_preprocess_latest_merge")

    # 모든값을 반으로 나눈후 반올림
    df_naver['score'] = round(df_naver['score'] / 2)

    # 0값이 있는 경우 1로 변경
    df_naver.loc[df_naver['score'] == 0, 'score'] = 1

    # score float > int 변경
    df_dtype_int = df_naver.astype({'score': int})

    # 저장
    csv_save(df_dtype_int, 'naver_reiview_preprocess_latest_merge')
========================================================================================================================
* 1점을 2로 나눈후 round(0.5) == 0 현상 발생 > 0점을 1점으로 변경




# 기록 3
=======================================================================================================================
* 사용한 코드
    df_all = csv_reader(file_name) # csv 파일 load
    df_preprocess = df_all.loc[:, ['score','preprocessed_review']] # 점수와 전처리된 리뷰만 가져옴
    df_clean = df_preprocess.dropna(axis=0) # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True) # 인덱스 재정렬

* 결과
    ssh://ubuntu@133.186.151.104:22/home/ubuntu/anaconda3/envs/tf2_py38_DEC/bin/python3.8 -u /tmp/pycharm_project_367/DEC-keras/datasets.py
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 3001 entries, 0 to 3000
    Data columns (total 2 columns):
     #   Column               Non-Null Count  Dtype
    ---  ------               --------------  -----
     0   score                3001 non-null   int64
     1   preprocessed_review  3001 non-null   object
    dtypes: int64(1), object(1)
    memory usage: 47.0+ KB
          score                                preprocessed_review
    0         4                                                 좋음
    1         4                                                 좋음
    2         5                                       전체적으로 만족도 높음
    3         2                                               그럭저럭
    4         4                                                 최고
    ...     ...                                                ...
    2996      2  하루 묵기는 무난합니다 중앙난방인 지방에 냉방을 요청했는데 난방 시즌이어서 냉방은 ...
    2997      2  박일 숙박했는데 너무 깔끔하고 전망도 아주 좋았습니다 층 디럭스룸 배정받았는데 웬만...
    2998      2  편히 쉬다 가기 좋은 곳 위치도 좋고 무엇보다 침구류가 좋아서 편안하게 휴식 취할 ...
    2999      2  멤버십 가입 후 처음 이용했는데 정말 퇴실하기 싫을 정도로 좋았습니다 분이 친절하게...
    3000      2  한국 갈 때마다 롯데시티나 신라스테이를 이용했는데 왜 글래드를 몰랐는지 아쉽다 앞으...

    [3001 rows x 2 columns]
    None
=======================================================================================================================
* 5000개의 리뷰 데이터가 있는 csv 파일에서 nan 값을 제외한 3000개의 데이터를 남겼다.



# 기록 4 (okt 까지 한번 돌려봄)
=======================================================================================================================
* 사용한 코드
    '''
    데이터 로딩
    '''
    print('Loading data...') # 데이터 로딩
    df_all = csv_reader(file_name) # csv 파일 load
    df_preprocess = df_all.loc[:, ['score','preprocessed_review']] # 점수와 전처리된 리뷰만 가져옴
    df_clean = df_preprocess.dropna(axis=0) # nan 값이 있는 행 삭제
    df_reindex = df_clean.reset_index(drop=True) # 인덱스 재정렬
    x1, y1 = df_reindex['preprocessed_review'], df_reindex['score'] # x 리뷰, y 점수

    '''
    okt
    '''
    print('Vectorizing sequence data...') # 데이터 벡터화
    tokenizer = okt_morph(x1) # 전처리된 리뷰 데이터 토크나이징
    return tokenizer


* 결과
    Loading data...
    Vectorizing sequence data...
    100%|██████████████████████████████████████| 3001/3001 [00:14<00:00, 211.03it/s]
    Traceback (most recent call last):
      File "/tmp/pycharm_project_367/DEC-keras/datasets.py", line 389, in <module>
        x, y = load_crawleing_reviews("naver_review_test_data")
    ValueError: too many values to unpack (expected 2)
=======================================================================================================================
* okt_morph() 함수에 시리즈 말고 데이터프레임으로 넣어야 될듯???


# 기록 5
=======================================================================================================================
* 사용한 코드
    # 형태소 분석 함수
    def okt_morph(dataframe):
        # 범위 지정 가능
        df_pre = dataframe['preprocessed_review']
        df_corpus = df_pre[:]

        clean_words = []
        for i in tqdm(df_corpus):
            ok = okt.pos(i)
            words = []
            for word in ok:
                if word[1] not in ['Josa', 'Eomi', 'Punctuation', 'Suffix']:  # 조사, 어미, 구두점이 있는 것은 포함 시키지 않음
                    words.append(word[0])
            clean_words.append(words)

        return clean_words

* 결과
    100%|██████████████████████████████████████| 3001/3001 [00:15<00:00, 196.22it/s]
    리뷰 데이터 : [['좋음'], ['좋음'], ['전체', '만족도', '높음'], ['그럭저럭'], ['최고'], ['서울', '내', '어디', '사',
    '통', '팔', '달', '교통', '편리'], ['강력', '추천'], ['매우', '나쁨'], ['구', '성비', '갑', '최고', '호텔'], .. 생략
=======================================================================================================================
* okt 형태소 분류 까지 완료


# 기록 6 (word2vec 예제파일을 따라하며 데이터가 출력되는지 확인해보기)
           * "좋음" 과 비슷한 단어 추출하기
========================================================================================================================
* 사용한 코드
    '''
    word2vec
    '''
    print('Vectorizing sequence data...')  # 데이터 벡터화
    model = Word2Vec(sentences=tokenizer, size=100, window=5, min_count=5, workers=4, sg=0)
    model_result = model.wv.most_similar("좋음")
    print(model_result)


* 결과
    Vectorizing sequence data...
    [('내부', 0.9974992275238037), ('좋습니다', 0.9973698854446411), ('깔끔했고', 0.9973666667938232), ('또한', 0.997318685054779),
    ('친절합니다', 0.9972794651985168), ('넓은', 0.9972636699676514), ('교통', 0.9972415566444397), ('접근성', 0.9972261190414429),
    ('보', 0.9972063899040222), ('방도', 0.9972002506256104)]
    리뷰 데이터 : None
========================================================================================================================
* 데이터가 올바르게 출력되었다. 하지만 word2vec에 대한 이해도가 낮은 상태에서 코드만 넣어서 확인한것이기 때문에
word2vec 공부를 해야함.






# 기록 7 ( 훈련된 모델 shape 확인 )
========================================================================================================================
okt 형태소 분류...
100%|██████████████████████████████████████| 3001/3001 [00:14<00:00, 200.77it/s]
Vectorizing sequence data...
(2065, 100)
========================================================================================================================


# 기록 8 ( 리뷰 통계 : image dir에서 png 확인)
=======================================================================================================================
리뷰의 최대 길이 : 150
리뷰의 평균 길이 : 25.571476174608463
=======================================================================================================================



# 기록 9 ( "만족" 과 비슷한 단어 찾기)
=======================================================================================================================
* sg = 0, min_count = 5
[('많이', 0.9998157024383545), ('정도', 0.9998121857643127), ('사람', 0.9998056888580322), ('식사', 0.9998053312301636),
('된', 0.9998018145561218), ('온도', 0.9998000860214233), ('없고', 0.9997991323471069), ('분위기', 0.9997941851615906),
 ('깨끗한', 0.9997904300689697), ('되는', 0.9997903108596802)]

* sg = 0, min_count = 10
Vectorizing sequence data...
[('도', 0.9998130798339844), ('같은', 0.9998012781143188), ('건', 0.9997950792312622), ('수영장', 0.9997948408126831),
('않아', 0.999783456325531), ('음식', 0.9997806549072266), ('분위기', 0.9997782707214355), ('신경', 0.9997768402099609),
('굉장히', 0.9997766017913818), ('해서', 0.9997742772102356)]

* sg = 0, min_count = 15
[('하지만', 0.999809741973877), ('서', 0.9997804164886475), ('와인', 0.9997660517692566), ('이었습니다', 0.9997588992118835),
('느낌', 0.9997587203979492), ('사람', 0.9997577667236328), ('인테리어', 0.9997565746307373), ('소음', 0.9997557401657104),
('아이', 0.9997552633285522), ('내', 0.9997514486312866)]

* sg = 0, min_count = 20
[('분위기', 0.9997180700302124), ('깨끗한', 0.9997081160545349), ('만족스러웠습니다', 0.9996935129165649), ('깔끔한', 0.9996843338012695),
('와인', 0.9996829628944397), ('큰', 0.9996799230575562), ('면', 0.9996751546859741), ('배려', 0.9996702075004578),
('않지만', 0.9996699094772339), ('않은', 0.9996668100357056)]

* sg = 0, min_count = 25
[('와인', 0.9997572302818298), ('보고', 0.9997364282608032), ('이었습니다', 0.9997293949127197), ('맥주', 0.9997281432151794),
('굉장히', 0.9997214078903198), ('코로나', 0.9997187852859497), ('식사', 0.9997166395187378), ('걸', 0.9997084736824036),
('좋아', 0.9997069239616394), ('문제', 0.9997051954269409)]

* sg = 0, min_count = 30
[('전혀', 0.9997217655181885), ('식사', 0.9997193217277527), ('만족스러웠습니다', 0.9997138977050781), ('였습니다', 0.999710202217102),
('방이', 0.9996978044509888), ('딱', 0.9996961355209351), ('굉장히', 0.9996952414512634), ('않은', 0.9996923208236694),
('볼', 0.9996908903121948), ('제공', 0.9996904730796814)]

************* sg 값 변경 ************************

* sg = 1, min_count = 5
[('친절했고', 0.9958021640777588), ('물론', 0.9955589771270752), ('끔', 0.9953944683074951), ('깔', 0.994544267654419),
('나', 0.9944262504577637), ('친절하시니', 0.9941003918647766), ('친절하세요', 0.9938473105430603), ('만족했습니다', 0.9937354922294617),
('너무나', 0.993686854839325), ('훌륭한', 0.993354856967926)]

* sg = 1, min_count = 10
[('위생', 0.995941162109375), ('깨끗했고', 0.9959219098091125), ('쾌적한', 0.9956857562065125), ('심플', 0.9951194524765015),
('좋았음', 0.9949259757995605), ('훌륭하고', 0.993772029876709), ('고급스러운', 0.993500828742981), ('깨끗해', 0.9931552410125732),
('나', 0.9931240081787109), ('만족스럽습니다', 0.9929053783416748)]

* sg = 1, min_count = 15
[('쾌적한', 0.9949052333831787), ('물론', 0.9939712882041931), ('위생', 0.9927648305892944), ('편한', 0.9920718669891357),
('좋았음', 0.9917362928390503), ('끔', 0.9915804862976074), ('고', 0.9909809827804565), ('친절', 0.9909777641296387),
('고급스러운', 0.9908297061920166), ('정비', 0.9907188415527344)]

* sg = 1, min_count = 20
[('만족스럽습니다', 0.9956712126731873), ('나', 0.9946420192718506), ('쾌적한', 0.9943250417709351), ('편한', 0.9942020773887634),
('물론', 0.9931555390357971), ('보니', 0.9931144118309021), ('고', 0.9930259585380554), ('모던', 0.9926996231079102),
('자체', 0.9923812747001648), ('했고', 0.9920921325683594)]

* sg = 1, min_count = 25
[('보니', 0.9961726665496826), ('고', 0.9959455728530884), ('편', 0.9952077865600586), ('물론', 0.9934786558151245),
('딱', 0.9934673309326172), ('이었는데', 0.9930583834648132), ('않은', 0.9926679134368896), ('도시락', 0.9926109313964844),
('되지', 0.992536723613739), ('되고', 0.9915354251861572)]

* sg = 1, min_count = 30
[('모던', 0.994388222694397), ('대', 0.9939221143722534), ('않은', 0.9937033653259277), ('나', 0.9933357834815979),
('고', 0.9929693937301636), ('그럭저럭', 0.9909741878509521), ('가지', 0.9906245470046997), ('인상', 0.9905768632888794),
('이었는데', 0.9903640151023865), ('이상', 0.9902962446212769)]
========================================================================================================================
* sg = 0 인 경우 min_count 수치를 높을수록 유사성이 높았다.
* sg = 1 인 경우 min_count 수치가 높을수록 유사성이 낮았다.
min_count 수치가 높을수록 학습할수 있는 단어의 개수가 줄어들기 때문에 min_count 수치가 낮을수록 효율이 좋은 sg = 1 값을
사용하는것이 적절하다.





# 기록 10
========================================================================================================================
* imdb() 의 x, y 값
    Vectorizing sequence data...
    x_train shape: (50000, 1000)
    imdb_x 값 : [[0. 1. 1. ... 0. 0. 0.]
     [0. 1. 1. ... 0. 0. 0.]
     [0. 1. 1. ... 0. 0. 0.]
     ...
     [0. 1. 1. ... 0. 0. 0.]
     [0. 1. 1. ... 0. 0. 0.]
     [0. 1. 1. ... 0. 0. 0.]]

    imdb_y 값 : [1 0 0 ... 0 0 0]



word2vec : Word2Vec(vocab=546, size=100, alpha=0.025)
모델 shape : (546, 100)
모델 타입 : <class 'gensim.models.word2vec.Word2Vec'>


